import os
import glob
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMRegressor
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# ğŸ¯ ìµœì¢… ê¶Œì¥ ì „ëµ: LightGBM + Transformer ì•™ìƒë¸” ëª¨ë¸ (Phase 2: ëª¨ë¸ ê°œì„ )
# - LightGBM(í”¼ì²˜ ê¸°ë°˜)ê³¼ Transformer(ì‹œí€€ìŠ¤ ê¸°ë°˜) ëª¨ë¸ì„ ëª¨ë‘ í•™ìŠµ
# - ì˜ˆì¸¡ ì‹œ, ë‘ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ 50:50ìœ¼ë¡œ ë‹¨ìˆœ í‰ê· í•˜ì—¬ ì•™ìƒë¸”
# - ëŒ€íšŒ ê·œì¹™(28ì¼ ê³ ì • ìœˆë„ìš°, Data Leakage ë°©ì§€)ì„ ì—„ê²©íˆ ì¤€ìˆ˜
# ==============================================================================


# ì‹œë“œ ê³ ì •
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)


# ê³µíœ´ì¼ ë¦¬ìŠ¤íŠ¸ (ë„ë©”ì¸ ì§€ì‹)
holiday_dates = pd.to_datetime([
    "2023-01-01", "2023-01-21", "2023-01-22", "2023-01-23", "2023-01-24",
    "2023-03-01", "2023-05-05", "2023-05-27", "2023-05-29", "2023-06-06",
    "2023-08-15", "2023-09-28", "2023-09-29", "2023-09-30", "2023-10-03",
    "2023-10-09", "2023-12-25", "2024-01-01", "2024-02-09", "2024-02-10",
    "2024-02-11", "2024-02-12", "2024-03-01", "2024-04-10", "2024-05-05",
    "2024-05-06", "2024-05-15", "2024-06-06", "2024-08-15", "2024-09-16",
    "2024-09-17", "2024-09-18", "2024-10-03", "2024-10-09", "2024-12-25"
])

def is_korean_holiday(date):
    return int(date in holiday_dates)

# ==============================================================================
# Phase 2/3 ì—ì„œ í™œìš© ê°€ëŠ¥í•œ Transformer ëª¨ë¸ ì•„í‚¤í…ì²˜
# ==============================================================================
class TransformerModel(nn.Module):
    def __init__(self, input_dim=1, d_model=32, nhead=4, num_layers=2, output_dim=7):
        super().__init__()
        self.input_projection = nn.Linear(input_dim, d_model)
        
        pe = torch.zeros(28, d_model)
        position = torch.arange(0, 28).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pos_encoding', pe.unsqueeze(0))

        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=0.1, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.output_projection = nn.Sequential(
            nn.Linear(d_model * 28, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128, output_dim)
        )
    
    def forward(self, x):
        x = self.input_projection(x) + self.pos_encoding.to(x.device)
        encoded = self.transformer(x)
        output = self.output_projection(encoded.view(encoded.size(0), -1))
        return output

class RestaurantDemandPredictor:
    """
    LightGBMê³¼ Transformer ì•™ìƒë¸”ì„ ìœ„í•œ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸
    """
    def __init__(self, device):
        self.lgbm_models = {}
        self.lgbm_scalers = {}
        self.transformer_models = {}
        self.transformer_scalers = {}
        self.feature_cols = None
        self.device = device

    def create_28day_features(self, data_28days, last_date):
        """28ì¼ ë°ì´í„°ë¡œë¶€í„° feature ì¶”ì¶œ (Data Leakage ë°©ì§€)"""
        features = {}
        data_28days = np.array(data_28days)

        # 1. ê¸°ë³¸ í†µê³„ëŸ‰
        features['mean_sales'] = np.mean(data_28days)
        features['std_sales'] = np.std(data_28days)
        features['median_sales'] = np.median(data_28days)
        features['min_sales'] = np.min(data_28days)
        features['max_sales'] = np.max(data_28days)
        
        # 2. ì£¼ë³„ íŒ¨í„´ (4ì£¼ê°„)
        for week in range(4):
            week_data = data_28days[week*7:(week+1)*7]
            features[f'week_{week}_mean'] = np.mean(week_data)

        # 3. ìµœê·¼ ê²½í–¥ì„±
        features['last_7day_mean'] = np.mean(data_28days[-7:])
        features['recent_trend'] = np.mean(data_28days[-7:]) - np.mean(data_28days[-14:-7])

        # 4. ë„ë©”ì¸ ì§€ì‹ (ì¶”ë¡  ì‹œì ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ)
        # ë‹¤ìŒ 7ì¼ê°„ì˜ ìš”ì¼, ì£¼ë§, ê³µíœ´ì¼ ì •ë³´
        for i in range(1, 8):
            pred_date = last_date + pd.Timedelta(days=i)
            features[f'pred_day_{i}_weekday'] = pred_date.weekday()
            features[f'pred_day_{i}_is_weekend'] = 1 if pred_date.weekday() >= 5 else 0
            features[f'pred_day_{i}_is_holiday'] = is_korean_holiday(pred_date)
            
        return features

    def prepare_lgbm_training_data(self, full_data, dates):
        """LGBM í•™ìŠµì„ ìœ„í•´ 28ì¼ ìœˆë„ìš°ë¡œ í”¼ì²˜ ë°ì´í„° ìƒì„±"""
        X_list, y_list = [], []
        
        for i in range(len(full_data) - 34):
            input_data = full_data[i:i+28]
            target_data = full_data[i+28:i+35]
            last_date = dates[i+27]
            
            features = self.create_28day_features(input_data, last_date)
            X_list.append(features)
            y_list.append(target_data)
        
        X_df = pd.DataFrame(X_list)
        if self.feature_cols is None:
            self.feature_cols = X_df.columns
        return X_df, np.array(y_list)

    def prepare_transformer_training_data(self, full_data):
        """Transformer í•™ìŠµì„ ìœ„í•´ 28ì¼ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        X_seq, y_seq = [], []
        for i in range(len(full_data) - 34):
            X_seq.append(full_data[i:i+28])
            y_seq.append(full_data[i+28:i+35])
        return np.array(X_seq), np.array(y_seq)

    def train_lightgbm_model(self, X_train, y_train, menu_name):
        """7ì¼ ì˜ˆì¸¡ì„ ìœ„í•œ 7ê°œì˜ LightGBM ëª¨ë¸ í•™ìŠµ"""
        models, scalers = [], []
        for day in range(7):
            y_day = y_train[:, day]
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_train)
            model = LGBMRegressor(random_state=42, verbose=-1)
            model.fit(X_scaled, y_day)
            models.append(model)
            scalers.append(scaler)
        self.lgbm_models[menu_name] = models
        self.lgbm_scalers[menu_name] = scalers

    def train_transformer_model(self, X_train_seq, y_train_seq, menu_name, epochs=10, batch_size=16):
        """Transformer ëª¨ë¸ í•™ìŠµ"""
        scaler = StandardScaler()
        # ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ 1Dë¡œ í¼ì³ì„œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ í›„ ë‹¤ì‹œ 2Dë¡œ ë³µì›
        X_train_reshaped = X_train_seq.reshape(-1, 1)
        scaler.fit(X_train_reshaped)
        X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train_seq.shape)

        X_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(-1).to(self.device)
        y_tensor = torch.FloatTensor(y_train_seq).to(self.device)
        
        dataset = TensorDataset(X_tensor, y_tensor)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        model = TransformerModel().to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        model.train()
        for epoch in range(epochs):
            for X_batch, y_batch in loader:
                optimizer.zero_grad()
                output = model(X_batch)
                loss = criterion(output, y_batch)
                loss.backward()
                optimizer.step()

        self.transformer_models[menu_name] = model
        self.transformer_scalers[menu_name] = scaler

    def predict_7days_ensemble(self, data_28days, last_date, menu_name):
        """LGBMê³¼ Transformer ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•˜ì—¬ 7ì¼ ì˜ˆì¸¡"""
        # 1. LightGBM ì˜ˆì¸¡
        features = self.create_28day_features(data_28days, last_date)
        X_pred_lgbm = pd.DataFrame([features])[self.feature_cols]
        lgbm_preds = []
        for day in range(7):
            scaler = self.lgbm_scalers[menu_name][day]
            model = self.lgbm_models[menu_name][day]
            X_scaled = scaler.transform(X_pred_lgbm)
            pred = model.predict(X_scaled)[0]
            lgbm_preds.append(max(0, pred))
        
        # 2. Transformer ì˜ˆì¸¡
        scaler = self.transformer_scalers[menu_name]
        model = self.transformer_models[menu_name]
        model.eval()
        
        input_scaled = scaler.transform(np.array(data_28days).reshape(-1, 1)).reshape(1, 28, 1)
        input_tensor = torch.FloatTensor(input_scaled).to(self.device)
        
        with torch.no_grad():
            transformer_preds_raw = model(input_tensor).squeeze().cpu().numpy()
        
        transformer_preds = np.maximum(0, transformer_preds_raw)

        # 3. ì•™ìƒë¸” (ë‹¨ìˆœ í‰ê· )
        ensemble_preds = (np.array(lgbm_preds) + transformer_preds) / 2.0
        
        return ensemble_preds


# ==============================================================================
# Main Execution
# ==============================================================================
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    print("ë°ì´í„° ë¡œë“œ ì¤‘...")
    train_df = pd.read_csv('./data/train/train.csv')
    train_df['ë§¤ì¶œìˆ˜ëŸ‰'] = train_df['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)
    train_df['ì˜ì—…ì¼ì'] = pd.to_datetime(train_df['ì˜ì—…ì¼ì'])
    submission_df = pd.read_csv('./data/sample_submission.csv')

    predictor = RestaurantDemandPredictor(device)
    unique_menus = train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].unique()
    
    # 1. ë©”ë‰´ë³„ ëª¨ë¸ í•™ìŠµ
    for menu_name in tqdm(unique_menus, desc="ë©”ë‰´ë³„ ëª¨ë¸ í•™ìŠµ (LGBM+Transformer)"):
        menu_df = train_df[train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu_name].sort_values(by='ì˜ì—…ì¼ì')
        
        if len(menu_df) < 35: continue
            
        sales_data = menu_df['ë§¤ì¶œìˆ˜ëŸ‰'].values
        dates = menu_df['ì˜ì—…ì¼ì'].values
        
        # LGBM í•™ìŠµ
        X_train_lgbm, y_train_lgbm = predictor.prepare_lgbm_training_data(sales_data, dates)
        predictor.train_lightgbm_model(X_train_lgbm, y_train_lgbm, menu_name)

        # Transformer í•™ìŠµ
        X_train_tf, y_train_tf = predictor.prepare_transformer_training_data(sales_data)
        predictor.train_transformer_model(X_train_tf, y_train_tf, menu_name, epochs=15) # ì—í¬í¬ ì¦ê°€

    # 2. Test íŒŒì¼ë³„ ì˜ˆì¸¡
    print("\nì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì¤‘...")
    all_predictions = []
    test_paths = sorted(glob.glob('./data/test/*.csv'))
    
    for path in tqdm(test_paths, desc="Test íŒŒì¼ë³„ ì˜ˆì¸¡"):
        test_file_df = pd.read_csv(path)
        test_file_df['ì˜ì—…ì¼ì'] = pd.to_datetime(test_file_df['ì˜ì—…ì¼ì'])
        basename = os.path.basename(path).replace('.csv', '')
        
        for menu_name in test_file_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].unique():
            if menu_name not in predictor.lgbm_models or menu_name not in predictor.transformer_models:
                continue

            # 28ì¼ ì…ë ¥ ë°ì´í„° êµ¬ì„± (train ëë¶€ë¶„ + test)
            train_menu_df = train_df[train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu_name]
            
            # test íŒŒì¼ì˜ ì²« ë‚ ì§œì™€ train ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì‚¬ì´ì˜ gap ê³„ì‚°
            test_start_date = test_file_df['ì˜ì—…ì¼ì'].min()
            train_last_date = train_menu_df['ì˜ì—…ì¼ì'].max()
            
            # í•„ìš”í•œ ê³¼ê±° ë°ì´í„° ì¼ ìˆ˜ ê³„ì‚°
            days_needed_from_train = 28 - len(test_file_df)
            if days_needed_from_train <= 0:
                 # test íŒŒì¼ë§Œìœ¼ë¡œ 28ì¼ì´ ì±„ì›Œì§€ëŠ” ê²½ìš°
                 context_df = test_file_df.sort_values(by='ì˜ì—…ì¼ì').tail(28)
            else:
                # train ë°ì´í„°ì™€ test ë°ì´í„°ë¥¼ í•©ì³ 28ì¼ êµ¬ì„±
                historical_data = train_menu_df.sort_values(by='ì˜ì—…ì¼ì').tail(days_needed_from_train)
                context_df = pd.concat([historical_data, test_file_df])

            input_28days_data = context_df['ë§¤ì¶œìˆ˜ëŸ‰'].values
            last_date = context_df['ì˜ì—…ì¼ì'].max()
            
            # ì•™ìƒë¸” ì˜ˆì¸¡
            pred_7days = predictor.predict_7days_ensemble(input_28days_data, last_date, menu_name)
            
            # ê²°ê³¼ ì €ì¥
            for i, pred_val in enumerate(pred_7days):
                all_predictions.append({
                    'ì˜ì—…ì¼ì': f"{basename}+{i+1}ì¼",
                    'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': menu_name,
                    'ë§¤ì¶œìˆ˜ëŸ‰': pred_val
                })

    # 3. ì œì¶œ íŒŒì¼ ìƒì„±
    if all_predictions:
        pred_df = pd.DataFrame(all_predictions)
        submission_pivot = pred_df.pivot(index='ì˜ì—…ì¼ì', columns='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', values='ë§¤ì¶œìˆ˜ëŸ‰').reset_index()
        final_submission = pd.merge(submission_df[['ì˜ì—…ì¼ì']], submission_pivot, on='ì˜ì—…ì¼ì', how='left')
        final_submission = final_submission.fillna(0)
        final_submission = final_submission[submission_df.columns]
        
        output_filename = 'submission_ensemble_lgbm_tf.csv'
        final_submission.to_csv(output_filename, index=False)
        print(f"\n{output_filename} íŒŒì¼ ìƒì„± ì™„ë£Œ")
    else:
        print("ìƒì„±ëœ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    print("\n=== ğŸ† LGBM + Transformer ì•™ìƒë¸” ëª¨ë¸ ===")
    print("âœ… LightGBM(í”¼ì²˜)ê³¼ Transformer(ì‹œí€€ìŠ¤) ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· í•˜ì—¬ ì•™ìƒë¸”")
    print("âœ… ëŒ€íšŒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ë§ì¶¤í˜• ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ")
