import os
import random
import glob
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import VotingRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# --- ê³ ì • ì‹œë“œ ---
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
set_seed(42)

# --- ê³µíœ´ì¼ 2023~2024 ---
holidays_2023_2024 = [
    "2023-01-01", "2023-01-21", "2023-01-22", "2023-01-23", "2023-01-24",
    "2023-03-01", "2023-05-05", "2023-05-27", "2023-05-29", "2023-06-06",
    "2023-08-15", "2023-09-28", "2023-09-29", "2023-09-30", "2023-10-03", 
    "2023-10-09", "2023-12-25",
    "2024-01-01", "2024-02-09", "2024-02-10", "2024-02-11", "2024-02-12", 
    "2024-03-01", "2024-04-10", "2024-05-05", "2024-05-06", "2024-05-15", 
    "2024-06-06", "2024-08-15", "2024-09-16", "2024-09-17", "2024-09-18", 
    "2024-10-03", "2024-10-09", "2024-12-25",
]

# --- ì•ˆì „í•œ ë¼ë²¨ ì¸ì½”ë”© í•¨ìˆ˜ ---
def safe_label_encode(le, values):
    """ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë¼ë²¨ ì¸ì½”ë”©"""
    result = []
    for val in values.astype(str):
        try:
            result.append(le.transform([val])[0])
        except ValueError:
            result.append(-1)  # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ëŠ” -1ë¡œ ì²˜ë¦¬
    return np.array(result)

# --- ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”© ---
def safe_target_encoding(train_df, test_df, cat_cols, target_col, alpha=10, cv_folds=5):
    """ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”© (êµì°¨ ê²€ì¦ ì‚¬ìš©)"""
    print(f"íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš©: {cat_cols}")
    
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    target_encoders = {}
    
    for col in cat_cols:
        if col not in train_df.columns:
            continue
            
        print(f"  - {col} ì¸ì½”ë”© ì¤‘...")
        train_encoded = np.zeros(len(train_df))
        
        # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”©
        for train_idx, val_idx in kf.split(train_df):
            train_part = train_df.iloc[train_idx]
            val_part = train_df.iloc[val_idx]
            
            # ë² ì´ì§€ì•ˆ ìŠ¤ë¬´ë”©
            target_mean = train_part.groupby(col)[target_col].mean()
            global_mean = train_part[target_col].mean()
            count = train_part.groupby(col).size()
            
            smoothed = (target_mean * count + global_mean * alpha) / (count + alpha)
            train_encoded[val_idx] = val_part[col].map(smoothed).fillna(global_mean)
        
        train_df[f'{col}_target_enc'] = train_encoded
        
        # í…ŒìŠ¤íŠ¸ìš©: ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ê³„ì‚°
        target_mean = train_df.groupby(col)[target_col].mean()
        global_mean = train_df[target_col].mean()
        count = train_df.groupby(col).size()
        smoothed = (target_mean * count + global_mean * alpha) / (count + alpha)
        
        # ì¸ì½”ë” ì €ì¥
        target_encoders[col] = {
            'smoothed': smoothed.to_dict(),
            'global_mean': global_mean
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©
        if col in test_df.columns:
            test_df[f'{col}_target_enc'] = test_df[col].map(smoothed).fillna(global_mean)
        else:
            test_df[f'{col}_target_enc'] = global_mean
    
    return train_df, test_df, target_encoders

# --- ê³ ê¸‰ ì‹œê°„ í”¼ì²˜ ìƒì„± ---
def create_advanced_time_features(df):
    """ê³ ê¸‰ ì‹œê°„ í”¼ì²˜ ìƒì„±"""
    df['ì˜ì—…ì¼ì_dt'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
    
    # ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
    df['ë…„'] = df['ì˜ì—…ì¼ì_dt'].dt.year
    df['ì›”'] = df['ì˜ì—…ì¼ì_dt'].dt.month
    df['ì¼'] = df['ì˜ì—…ì¼ì_dt'].dt.day
    df['ìš”ì¼'] = df['ì˜ì—…ì¼ì_dt'].dt.dayofweek
    df['ì£¼ì°¨'] = df['ì˜ì—…ì¼ì_dt'].dt.isocalendar().week
    df['ë¶„ê¸°'] = df['ì˜ì—…ì¼ì_dt'].dt.quarter
    df['ë…„ì¤‘_ëª‡ì§¸ë‚ '] = df['ì˜ì—…ì¼ì_dt'].dt.dayofyear
    
    # ì‚¼ê°í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ì£¼ê¸°ì„± í‘œí˜„
    df['ì›”_sin'] = np.sin(2 * np.pi * df['ì›”'] / 12)
    df['ì›”_cos'] = np.cos(2 * np.pi * df['ì›”'] / 12)
    df['ìš”ì¼_sin'] = np.sin(2 * np.pi * df['ìš”ì¼'] / 7)
    df['ìš”ì¼_cos'] = np.cos(2 * np.pi * df['ìš”ì¼'] / 7)
    df['ì¼_sin'] = np.sin(2 * np.pi * df['ì¼'] / 31)
    df['ì¼_cos'] = np.cos(2 * np.pi * df['ì¼'] / 31)
    
    # ê³„ì ˆì„±
    df['ë´„'] = df['ì›”'].isin([3, 4, 5]).astype(int)
    df['ì—¬ë¦„'] = df['ì›”'].isin([6, 7, 8]).astype(int)
    df['ê°€ì„'] = df['ì›”'].isin([9, 10, 11]).astype(int)
    df['ê²¨ìš¸'] = df['ì›”'].isin([12, 1, 2]).astype(int)
    
    # íŠ¹ë³„í•œ ì›”
    df['1ì›”'] = (df['ì›”'] == 1).astype(int)
    df['3ì›”'] = (df['ì›”'] == 3).astype(int)
    df['5ì›”'] = (df['ì›”'] == 5).astype(int)
    df['8ì›”'] = (df['ì›”'] == 8).astype(int)
    df['12ì›”'] = (df['ì›”'] == 12).astype(int)
    
    # ì›”ì˜ íŠ¹ì„±
    df['ì›”ì´ˆ'] = (df['ì¼'] <= 5).astype(int)
    df['ì›”ì¤‘'] = ((df['ì¼'] > 5) & (df['ì¼'] <= 25)).astype(int)
    df['ì›”ë§'] = (df['ì¼'] > 25).astype(int)
    
    # ê¸‰ì—¬ì¼ íš¨ê³¼
    df['ê¸‰ì—¬ì¼ê·¼ì²˜'] = ((df['ì¼'] >= 23) & (df['ì¼'] <= 28)).astype(int)
    
    # ì£¼ë§/ê³µíœ´ì¼
    df['ì£¼ë§ì—¬ë¶€'] = (df['ìš”ì¼'] >= 5).astype(int)
    df['ê³µíœ´ì¼ì—¬ë¶€'] = df['ì˜ì—…ì¼ì_dt'].dt.strftime('%Y-%m-%d').isin(holidays_2023_2024).astype(int)
    df['ê¸ˆìš”ì¼'] = (df['ìš”ì¼'] == 4).astype(int)
    df['ì¼ìš”ì¼'] = (df['ìš”ì¼'] == 6).astype(int)
    
    return df

# --- ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ---
def create_interaction_features(df):
    """ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±"""
    # ì˜ì—…ì¥ Ã— ê³„ì ˆ
    seasons = ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']
    stores = ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']
    
    for season in seasons:
        for store in stores:
            if f'{season}' in df.columns and f'ì˜ì—…ì¥_{store}' in df.columns:
                df[f'{store}_{season}'] = df[f'{season}'] * df[f'ì˜ì—…ì¥_{store}']
    
    # ë©”ë‰´ ì¹´í…Œê³ ë¦¬ Ã— ìš”ì¼
    menu_categories = ['ìŒë£Œë¥˜', 'ì£¼ë¥˜ë¥˜', 'ë¶„ì‹ë¥˜', 'í•œì‹ë¥˜', 'ì–‘ì‹ë¥˜', 'ë””ì €íŠ¸ë¥˜']
    for category in menu_categories:
        if category in df.columns:
            df[f'{category}_ì£¼ë§'] = df[category] * df['ì£¼ë§ì—¬ë¶€']
            df[f'{category}_í‰ì¼'] = df[category] * (1 - df['ì£¼ë§ì—¬ë¶€'])
    
    # ì˜ì—…ì¥ Ã— ì£¼ë§
    for store in stores:
        if f'ì˜ì—…ì¥_{store}' in df.columns:
            df[f'{store}_ì£¼ë§'] = df[f'ì˜ì—…ì¥_{store}'] * df['ì£¼ë§ì—¬ë¶€']
    
    # ì›” Ã— ì£¼ë§
    special_months = [1, 3, 5, 8, 12]
    for month in special_months:
        if f'{month}ì›”' in df.columns:
            df[f'{month}ì›”_ì£¼ë§'] = df[f'{month}ì›”'] * df['ì£¼ë§ì—¬ë¶€']
    
    return df

# --- ì´ìƒì¹˜ ì²˜ë¦¬ ---
def handle_outliers_by_group(df, target_col='ë§¤ì¶œìˆ˜ëŸ‰'):
    """ê·¸ë£¹ë³„ ì´ìƒì¹˜ ì²˜ë¦¬"""
    if target_col not in df.columns:
        return df
    
    print("ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
    original_mean = df[target_col].mean()
    
    # ì˜ì—…ì¥ë³„ ì´ìƒì¹˜ ì²˜ë¦¬
    df[f'{target_col}_clipped'] = df.groupby('ì˜ì—…ì¥ëª…')[target_col].transform(
        lambda x: x.clip(lower=x.quantile(0.02), upper=x.quantile(0.98))
    )
    
    # ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
    processed_mean = df[f'{target_col}_clipped'].mean()
    print(f"  ì´ìƒì¹˜ ì²˜ë¦¬ ì „ í‰ê· : {original_mean:.2f}")
    print(f"  ì´ìƒì¹˜ ì²˜ë¦¬ í›„ í‰ê· : {processed_mean:.2f}")
    
    # ì›ë³¸ ëŒ€ì‹  ì²˜ë¦¬ëœ ê°’ ì‚¬ìš©
    df[target_col] = df[f'{target_col}_clipped']
    df = df.drop(f'{target_col}_clipped', axis=1)
    
    return df

# --- ê°œì„ ëœ í”¼ì²˜ ìƒì„± ---
def create_improved_features(df, is_train=True, encoders=None, target_encoders=None, train_df_for_target=None):
    """ê°œì„ ëœ í”¼ì²˜ ìƒì„±"""
    df = df.copy()
    
    # 1. ê³ ê¸‰ ì‹œê°„ í”¼ì²˜
    df = create_advanced_time_features(df)
    
    # 2. ì˜ì—…ì¥/ë©”ë‰´ ë¶„ë¦¬
    df['ì˜ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_').str[0]
    df['ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_', n=1).str[1].fillna('')
    
    # 3. ì˜ì—…ì¥ë³„ ì›í•« ì¸ì½”ë”©
    unique_stores = ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']
    for store in unique_stores:
        df[f'ì˜ì—…ì¥_{store}'] = (df['ì˜ì—…ì¥ëª…'] == store).astype(int)
    
    # 4. ë©”ë‰´ ì¹´í…Œê³ ë¦¬
    df['ìŒë£Œë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ì•„ë©”ë¦¬ì¹´ë…¸|ë¼ë–¼|ì½œë¼|ìŠ¤í”„ë¼ì´íŠ¸|ìƒìˆ˜|ì—ì´ë“œ|ìŒë£Œ', na=False).astype(int)
    df['ì£¼ë¥˜ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ë§‰ê±¸ë¦¬|ì†Œì£¼|ë§¥ì£¼|ì™€ì¸|ì¹µí…Œì¼|í•˜ì´ë³¼', na=False).astype(int)
    df['ë¶„ì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ë–¡ë³¶ì´|íŠ€ê¹€|í•«ë„ê·¸|ì–´ë¬µ|ê¼¬ì¹˜', na=False).astype(int)
    df['í•œì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('êµ­ë°¥|ë¶ˆê³ ê¸°|ê¹€ì¹˜|ëœì¥|ë¹„ë¹”ë°¥|í•œì‹', na=False).astype(int)
    df['ì–‘ì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('íŒŒìŠ¤íƒ€|í”¼ì|ìŠ¤í…Œì´í¬|ìƒëŸ¬ë“œ|ì–‘ì‹', na=False).astype(int)
    df['ë””ì €íŠ¸ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ì¼€ì´í¬|ë¹µ|ê³¼ì|ë””ì €íŠ¸|ì•„ì´ìŠ¤í¬ë¦¼', na=False).astype(int)
    df['ì„¸íŠ¸ë©”ë‰´'] = df['ë©”ë‰´ëª…'].str.contains('ì„¸íŠ¸|íŒ¨í‚¤ì§€|ì½¤ë³´', na=False).astype(int)
    
    # 5. ìƒí˜¸ì‘ìš© í”¼ì²˜
    df = create_interaction_features(df)
    
    # 6. ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    categorical_features = ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
    
    if is_train:
        # í›ˆë ¨ ì‹œ: ìƒˆë¡œìš´ ì¸ì½”ë” ìƒì„±
        if encoders is None:
            encoders = {}
        for col in categorical_features:
            if col in df.columns:
                le = LabelEncoder()
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
                encoders[col] = le
        
        # ì´ìƒì¹˜ ì²˜ë¦¬
        df = handle_outliers_by_group(df, 'ë§¤ì¶œìˆ˜ëŸ‰')
        
        return df, encoders
    else:
        # í…ŒìŠ¤íŠ¸ ì‹œ: ê¸°ì¡´ ì¸ì½”ë” ì‚¬ìš©
        for col in categorical_features:
            if col in df.columns and encoders and col in encoders:
                df[f'{col}_encoded'] = safe_label_encode(encoders[col], df[col])
            else:
                df[f'{col}_encoded'] = -1
        
        # íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš© (í…ŒìŠ¤íŠ¸)
        if target_encoders and train_df_for_target is not None:
            target_cols = ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
            _, df, _ = safe_target_encoding(train_df_for_target, df, target_cols, 'ë§¤ì¶œìˆ˜ëŸ‰')
        
        return df

# --- ê³ ê¸‰ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ---
def analyze_feature_importance_comprehensive(models, feature_names, train_df, target_col):
    """í¬ê´„ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ” í¬ê´„ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„")
    print("="*60)
    
    importance_results = {}
    
    # 1. ê° ëª¨ë¸ë³„ í”¼ì²˜ ì¤‘ìš”ë„
    for model_name, model in models.items():
        if hasattr(model, 'feature_importances_'):
            importance_results[model_name] = model.feature_importances_
    
    # 2. í†µê³„ì  ì¤‘ìš”ë„ (F-score)
    X = train_df[feature_names]
    y = train_df[target_col]
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in feature_names:
        if col.endswith('_encoded'):
            X[col] = X[col].fillna(-1)
        else:
            X[col] = X[col].fillna(0)
    
    f_scores, _ = f_regression(X, y)
    importance_results['f_score'] = f_scores
    
    # 3. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„
    correlations = []
    for col in feature_names:
        if col in X.columns:
            corr = abs(X[col].corr(y))
            correlations.append(corr)
        else:
            correlations.append(0)
    importance_results['correlation'] = np.array(correlations)
    
    # 4. ì¢…í•© ì¤‘ìš”ë„ ê³„ì‚° (ê°€ì¤‘í‰ê· )
    weights = {
        'catboost': 0.35,
        'lightgbm': 0.3,
        'xgboost': 0.25,
        'f_score': 0.1
    }
    
    # ì •ê·œí™”
    normalized_importance = {}
    for method, scores in importance_results.items():
        if method != 'correlation':
            # min-max ì •ê·œí™”
            scores_norm = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
            normalized_importance[method] = scores_norm
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    combined_score = np.zeros(len(feature_names))
    for method, weight in weights.items():
        if method in normalized_importance:
            combined_score += weight * normalized_importance[method]
    
    # í”¼ì²˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'combined_score': combined_score,
        'catboost_importance': importance_results.get('catboost', np.zeros(len(feature_names))),
        'lightgbm_importance': importance_results.get('lightgbm', np.zeros(len(feature_names))),
        'xgboost_importance': importance_results.get('xgboost', np.zeros(len(feature_names))),
        'f_score': importance_results.get('f_score', np.zeros(len(feature_names))),
        'correlation': importance_results.get('correlation', np.zeros(len(feature_names)))
    }).sort_values('combined_score', ascending=False).reset_index(drop=True)
    
    # í”¼ì²˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    def categorize_feature(feature_name):
        if 'target_enc' in feature_name:
            return 'ğŸ¯ íƒ€ê²Ÿ ì¸ì½”ë”©'
        elif 'encoded' in feature_name:
            return 'ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©'
        elif any(store in feature_name for store in ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']):
            if '_' in feature_name and feature_name.count('_') > 1:
                return 'ğŸ”— ì˜ì—…ì¥ ìƒí˜¸ì‘ìš©'
            else:
                return 'ğŸª ì˜ì—…ì¥'
        elif any(menu in feature_name for menu in ['ìŒë£Œë¥˜', 'ì£¼ë¥˜ë¥˜', 'ë¶„ì‹ë¥˜', 'í•œì‹ë¥˜', 'ì–‘ì‹ë¥˜', 'ë””ì €íŠ¸ë¥˜', 'ì„¸íŠ¸ë©”ë‰´']):
            if '_' in feature_name and feature_name.count('_') > 0:
                return 'ğŸ”— ë©”ë‰´ ìƒí˜¸ì‘ìš©'
            else:
                return 'ğŸ½ï¸ ë©”ë‰´ ì¹´í…Œê³ ë¦¬'
        elif feature_name in ['ë…„', 'ì›”', 'ì¼', 'ìš”ì¼', 'ì£¼ì°¨', 'ë¶„ê¸°', 'ë…„ì¤‘_ëª‡ì§¸ë‚ ']:
            return 'ğŸ“… ê¸°ë³¸ ì‹œê°„'
        elif 'sin' in feature_name or 'cos' in feature_name:
            return 'ã€°ï¸ ì‚¼ê°í•¨ìˆ˜'
        elif feature_name in ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']:
            return 'ğŸŒ± ê³„ì ˆì„±'
        elif feature_name in ['1ì›”', '3ì›”', '5ì›”', '8ì›”', '12ì›”']:
            return 'ğŸ“† íŠ¹ë³„í•œ ì›”'
        elif feature_name in ['ì›”ì´ˆ', 'ì›”ì¤‘', 'ì›”ë§', 'ê¸‰ì—¬ì¼ê·¼ì²˜']:
            return 'ğŸ“Š ì›” íŠ¹ì„±'
        elif feature_name in ['ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€', 'ê¸ˆìš”ì¼', 'ì¼ìš”ì¼']:
            return 'ğŸ‰ ì£¼ë§/ê³µíœ´ì¼'
        else:
            return 'â“ ê¸°íƒ€'
    
    feature_importance_df['category'] = feature_importance_df['feature'].apply(categorize_feature)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì „ì²´ í”¼ì²˜ ì¤‘ìš”ë„ ìˆœìœ„ (Top 30)")
    print("-" * 100)
    print(f"{'ìˆœìœ„':>3} {'í”¼ì²˜ëª…':<35} {'ì¹´í…Œê³ ë¦¬':<20} {'ì¢…í•©ì ìˆ˜':>8} {'CatBoost':>8} {'LightGBM':>8} {'XGBoost':>8}")
    print("-" * 100)
    
    for idx, row in feature_importance_df.head(30).iterrows():
        print(f"{idx+1:>3} {row['feature']:<35} {row['category']:<20} "
              f"{row['combined_score']:>8.4f} {row['catboost_importance']:>8.4f} "
              f"{row['lightgbm_importance']:>8.4f} {row['xgboost_importance']:>8.4f}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ìš”ì•½
    print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ìš”ì•½")
    print("-" * 50)
    category_importance = feature_importance_df.groupby('category').agg({
        'combined_score': ['mean', 'sum', 'count']
    }).round(4)
    category_importance.columns = ['í‰ê· _ì¤‘ìš”ë„', 'ì´í•©_ì¤‘ìš”ë„', 'í”¼ì²˜_ìˆ˜']
    category_importance = category_importance.sort_values('ì´í•©_ì¤‘ìš”ë„', ascending=False)
    
    for category, row in category_importance.iterrows():
        print(f"{category:<25} í‰ê· :{row['í‰ê· _ì¤‘ìš”ë„']:>7.4f} ì´í•©:{row['ì´í•©_ì¤‘ìš”ë„']:>7.4f} ê°œìˆ˜:{row['í”¼ì²˜_ìˆ˜']:>3.0f}")
    
    # ì¶”ì²œ í”¼ì²˜ ì„ íƒ
    print(f"\nğŸ¯ ì¶”ì²œ í”¼ì²˜ ì„ íƒ ì „ëµ")
    print("-" * 50)
    
    # Top K í”¼ì²˜ë“¤ ì„ íƒ ì „ëµ
    strategies = {
        "ë³´ìˆ˜ì  (Top 30)": 30,
        "ê· í˜•ì  (Top 40)": 40, 
        "ê³µê²©ì  (Top 50)": 50
    }
    
    for strategy_name, k in strategies.items():
        selected_features = feature_importance_df.head(k)['feature'].tolist()
        category_dist = selected_features = feature_importance_df.head(k)['category'].value_counts()
        
        print(f"\n{strategy_name}:")
        print(f"  ì„ íƒ í”¼ì²˜ ìˆ˜: {k}ê°œ")
        print(f"  ì£¼ìš” ì¹´í…Œê³ ë¦¬ êµ¬ì„±:")
        for cat, count in category_dist.head(5).items():
            print(f"    - {cat}: {count}ê°œ")
    
    return feature_importance_df

# --- í”¼ì²˜ ì„ íƒ ëª¨ë¸ í•™ìŠµ ---
def train_feature_selected_model(train_df, target_col='ë§¤ì¶œìˆ˜ëŸ‰', n_features=40):
    """í”¼ì²˜ ì„ íƒ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ"""
    
    print("=== í”¼ì²˜ ì„ íƒ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ===")
    print(f"ëª©í‘œ: 90ê°œ â†’ {n_features}ê°œ í•µì‹¬ í”¼ì²˜ ì„ íƒ")
    
    # í”¼ì²˜ ìƒì„±
    print("í”¼ì²˜ ìƒì„± ì¤‘...")
    train_df, encoders = create_improved_features(train_df, is_train=True)
    
    # íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš©
    target_cols = ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
    dummy_test = train_df.head(1).copy()
    train_df, _, target_encoders = safe_target_encoding(train_df, dummy_test, target_cols, target_col)
    
    # ì „ì²´ í”¼ì²˜ ëª©ë¡
    feature_columns = [
        # ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
        'ë…„', 'ì›”', 'ì¼', 'ìš”ì¼', 'ì£¼ì°¨', 'ë¶„ê¸°', 'ë…„ì¤‘_ëª‡ì§¸ë‚ ',
        # ì‚¼ê°í•¨ìˆ˜ í”¼ì²˜
        'ì›”_sin', 'ì›”_cos', 'ìš”ì¼_sin', 'ìš”ì¼_cos', 'ì¼_sin', 'ì¼_cos',
        # ê³„ì ˆì„±
        'ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸',
        # íŠ¹ë³„í•œ ì›”
        '1ì›”', '3ì›”', '5ì›”', '8ì›”', '12ì›”',
        # ì›”ì˜ íŠ¹ì„±
        'ì›”ì´ˆ', 'ì›”ì¤‘', 'ì›”ë§', 'ê¸‰ì—¬ì¼ê·¼ì²˜',
        # ì£¼ë§/ê³µíœ´ì¼
        'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€', 'ê¸ˆìš”ì¼', 'ì¼ìš”ì¼',
        # ì˜ì—…ì¥
        'ì˜ì—…ì¥_í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì˜ì—…ì¥_ì¹´í˜í…Œë¦¬ì•„', 'ì˜ì—…ì¥_í™”ë‹´ìˆ²ì£¼ë§‰', 'ì˜ì—…ì¥_ë‹´í•˜', 'ì˜ì—…ì¥_ë¯¸ë¼ì‹œì•„',
        # ë©”ë‰´ ì¹´í…Œê³ ë¦¬
        'ìŒë£Œë¥˜', 'ì£¼ë¥˜ë¥˜', 'ë¶„ì‹ë¥˜', 'í•œì‹ë¥˜', 'ì–‘ì‹ë¥˜', 'ë””ì €íŠ¸ë¥˜', 'ì„¸íŠ¸ë©”ë‰´',
        # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
        'ì˜ì—…ì¥ëª…_encoded', 'ë©”ë‰´ëª…_encoded', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…_encoded',
        # íƒ€ê²Ÿ ì¸ì½”ë”©
        'ì˜ì—…ì¥ëª…_target_enc', 'ë©”ë‰´ëª…_target_enc', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…_target_enc'
    ]
    
    # ìƒí˜¸ì‘ìš© í”¼ì²˜ë“¤ ì¶”ê°€
    interaction_features = [col for col in train_df.columns if '_ì£¼ë§' in col or '_ë´„' in col or '_ì—¬ë¦„' in col or '_ê°€ì„' in col or '_ê²¨ìš¸' in col or '_í‰ì¼' in col]
    feature_columns.extend(interaction_features)
    
    # ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì„ íƒ
    available_features = [col for col in feature_columns if col in train_df.columns]
    print(f"ì „ì²´ ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ ìˆ˜: {len(available_features)}")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in available_features:
        if col.endswith('_encoded'):
            train_df[col] = train_df[col].fillna(-1)
        else:
            train_df[col] = train_df[col].fillna(0)
    
    # ê°œë³„ ëª¨ë¸ë“¤ ìƒì„± (í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ìš©)
    individual_models = {
        'catboost': CatBoostRegressor(
            iterations=300, learning_rate=0.08, depth=7, l2_leaf_reg=2,
            random_seed=42, verbose=False
        ),
        'lightgbm': lgb.LGBMRegressor(
            n_estimators=300, learning_rate=0.08, max_depth=7, num_leaves=31,
            subsample=0.8, colsample_bytree=0.8, random_state=42, verbose=-1
        ),
        'xgboost': xgb.XGBRegressor(
            n_estimators=300, learning_rate=0.08, max_depth=7,
            subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0
        )
    }
    
    # ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ (í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ìš©)
    X_full = train_df[available_features]
    y_full = train_df[target_col]
    
    print("\ní”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ì„ ìœ„í•œ ëª¨ë¸ í•™ìŠµ...")
    trained_models = {}
    cat_features = [i for i, col in enumerate(available_features) if col.endswith('_encoded')]
    
    for name, model in individual_models.items():
        if name == 'catboost':
            model.fit(X_full, y_full, cat_features=cat_features)
        else:
            model.fit(X_full, y_full)
        trained_models[name] = model
    
    # í¬ê´„ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    feature_importance_df = analyze_feature_importance_comprehensive(
        trained_models, available_features, train_df, target_col
    )
    
    # Top K í”¼ì²˜ ì„ íƒ
    selected_features = feature_importance_df.head(n_features)['feature'].tolist()
    print(f"\nâœ… ì„ íƒëœ {len(selected_features)}ê°œ í•µì‹¬ í”¼ì²˜:")
    
    # ì„ íƒëœ í”¼ì²˜ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¶œë ¥
    selected_df = feature_importance_df.head(n_features)
    for category in selected_df['category'].unique():
        category_features = selected_df[selected_df['category'] == category]['feature'].tolist()
        print(f"  {category}: {len(category_features)}ê°œ")
        for feature in category_features[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            rank = selected_df[selected_df['feature'] == feature].index[0] + 1
            score = selected_df[selected_df['feature'] == feature]['combined_score'].iloc[0]
            print(f"    {rank:2d}. {feature:<30} (ì ìˆ˜: {score:.4f})")
        if len(category_features) > 3:
            print(f"    ... ì™¸ {len(category_features)-3}ê°œ")
    
    # ì„ íƒëœ í”¼ì²˜ë¡œ êµì°¨ ê²€ì¦
    print(f"\n=== í”¼ì²˜ ì„ íƒ ëª¨ë¸ êµì°¨ ê²€ì¦ ===")
    tscv = TimeSeriesSplit(n_splits=5)
    
    X_selected = train_df[selected_features]
    y = train_df[target_col]
    
    # ê²°ì¸¡ì¹˜ ì¬ì²˜ë¦¬
    for col in selected_features:
        if col.endswith('_encoded'):
            X_selected[col] = X_selected[col].fillna(-1)
        else:
            X_selected[col] = X_selected[col].fillna(0)
    
    model_scores = {}
    ensemble_predictions = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_selected)):
        print(f"\nFold {fold + 1} í‰ê°€ ì¤‘...")
        
        X_train_fold = X_selected.iloc[train_idx]
        y_train_fold = y.iloc[train_idx]
        X_val_fold = X_selected.iloc[val_idx]
        y_val_fold = y.iloc[val_idx]
        
        fold_predictions = {}
        
        # ì„ íƒëœ í”¼ì²˜ì—ì„œ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ì¬ì •ì˜
        selected_cat_features = [i for i, col in enumerate(selected_features) if col.endswith('_encoded')]
        
        for name, model in individual_models.items():
            if name == 'catboost':
                model.fit(X_train_fold, y_train_fold, cat_features=selected_cat_features)
            else:
                model.fit(X_train_fold, y_train_fold)
            
            pred = model.predict(X_val_fold)
            pred = np.maximum(pred, 0)
            
            rmse = np.sqrt(np.mean((y_val_fold - pred) ** 2))
            fold_predictions[name] = pred
            
            if name not in model_scores:
                model_scores[name] = []
            model_scores[name].append(rmse)
            
            print(f"  {name:>10}: {rmse:.4f}")
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_pred = (
            0.4 * fold_predictions['catboost'] +
            0.35 * fold_predictions['lightgbm'] +
            0.25 * fold_predictions['xgboost']
        )
        
        ensemble_rmse = np.sqrt(np.mean((y_val_fold - ensemble_pred) ** 2))
        ensemble_predictions.append(ensemble_rmse)
        
        print(f"  {'ì•™ìƒë¸”':>10}: {ensemble_rmse:.4f}")
    
    # ì„±ëŠ¥ ìš”ì•½
    print(f"\n=== í”¼ì²˜ ì„ íƒ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===")
    for name, scores in model_scores.items():
        print(f"{name:>12}: í‰ê·  {np.mean(scores):.4f} (Â±{np.std(scores):.4f})")
    
    ensemble_mean = np.mean(ensemble_predictions)
    ensemble_std = np.std(ensemble_predictions)
    print(f"{'ì•™ìƒë¸”':>12}: í‰ê·  {ensemble_mean:.4f} (Â±{ensemble_std:.4f})")
    print(f"{'ì•ˆì •ì„± ì§€ìˆ˜':>12}: {ensemble_std/ensemble_mean:.3f}")
    
    # ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
    previous_score = 22.71  # ì´ì „ ì•™ìƒë¸” ëª¨ë¸ ê²°ê³¼
    improvement = previous_score - ensemble_mean
    improvement_pct = (improvement / previous_score) * 100
    
    print(f"\nğŸ‰ í”¼ì²˜ ì„ íƒ íš¨ê³¼:")
    print(f"  ì´ì „ ëª¨ë¸ (90ê°œ í”¼ì²˜): {previous_score:.2f}")
    print(f"  ì„ íƒ ëª¨ë¸ ({n_features}ê°œ í”¼ì²˜): {ensemble_mean:.2f}")
    print(f"  ì„±ëŠ¥ ë³€í™”: {improvement:+.2f} ({improvement_pct:+.1f}%)")
    print(f"  í”¼ì²˜ ê°ì†Œ: 90ê°œ â†’ {n_features}ê°œ ({(90-n_features)/90*100:.0f}% ê°ì†Œ)")
    
    # ìµœì¢… ëª¨ë¸ í•™ìŠµ
    print(f"\nìµœì¢… í”¼ì²˜ ì„ íƒ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    final_models = {}
    
    for name, model in individual_models.items():
        print(f"  {name} ìµœì¢… í•™ìŠµ ì¤‘...")
        if name == 'catboost':
            model.fit(X_selected, y, cat_features=selected_cat_features)
        else:
            model.fit(X_selected, y)
        final_models[name] = model
    
    return final_models, encoders, selected_features, target_encoders, train_df, feature_importance_df

# --- í”¼ì²˜ ì„ íƒ ì˜ˆì¸¡ í•¨ìˆ˜ ---
def predict_feature_selected(test_df, models, encoders, feature_columns, target_encoders, train_df_for_target):
    """í”¼ì²˜ ì„ íƒ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
    
    # í”¼ì²˜ ìƒì„±
    test_df = create_improved_features(test_df, is_train=False, encoders=encoders, 
                                     target_encoders=target_encoders, train_df_for_target=train_df_for_target)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in feature_columns:
        if col in test_df.columns:
            if col.endswith('_encoded'):
                test_df[col] = test_df[col].fillna(-1)
            else:
                test_df[col] = test_df[col].fillna(0)
        else:
            test_df[col] = 0
    
    # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
    predictions = {}
    for name, model in models.items():
        pred = model.predict(test_df[feature_columns])
        pred = np.maximum(pred, 0)
        predictions[name] = pred
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = (
        0.4 * predictions['catboost'] +
        0.35 * predictions['lightgbm'] +
        0.25 * predictions['xgboost']
    )
    
    return ensemble_pred

# --- ì œì¶œ íŒŒì¼ ë³€í™˜ ---
def convert_to_submission_format(pred_df, sample_submission):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    def convert_to_integer(value):
        return max(0, int(round(value)))
    
    pred_dict = dict(zip(zip(pred_df['ì˜ì—…ì¼ì'].astype(str), pred_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].astype(str)), pred_df['ë§¤ì¶œìˆ˜ëŸ‰']))
    final_df = sample_submission.copy()
    
    for idx in final_df.index:
        date = str(final_df.loc[idx, 'ì˜ì—…ì¼ì'])
        for col in final_df.columns[1:]:
            val = pred_dict.get((date, str(col)), 0)
            final_df.loc[idx, col] = convert_to_integer(val)
    
    return final_df

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("=== ğŸ¯ í”¼ì²˜ ì„ íƒ ìµœì í™” ëª¨ë¸ ===")
    print("ëª©í‘œ:")
    print("  ğŸ“Š í”¼ì²˜ ìˆ˜: 90ê°œ â†’ 40ê°œ (í•µì‹¬ë§Œ ì„ íƒ)")
    print("  ğŸ“ˆ ì„±ëŠ¥: ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¡œ ì•ˆì •ì„± í–¥ìƒ")
    print("  ğŸ” í•´ì„ë ¥: ì¤‘ìš” í”¼ì²˜ ëª…í™•í•œ ë­í‚¹ ì œê³µ")
    print("  ğŸš€ íš¨ìœ¨ì„±: í•™ìŠµ/ì˜ˆì¸¡ ì†ë„ í–¥ìƒ")
    print()

    # 1. ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë“œ ì¤‘...")
    train = pd.read_csv('./data/train/train.csv')
    print(f"í›ˆë ¨ ë°ì´í„°: {train.shape}")

    # 2. í”¼ì²˜ ì„ íƒ ëª¨ë¸ í•™ìŠµ
    print("\ní”¼ì²˜ ì„ íƒ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    models, encoders, selected_features, target_encoders, processed_train, feature_importance_df = train_feature_selected_model(train, n_features=40)

    # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    all_preds = []
    
    test_files = sorted(glob.glob('./data/test/TEST_*.csv'))
    for i, path in enumerate(test_files):
        print(f"ì²˜ë¦¬ ì¤‘ ({i+1}/{len(test_files)}): {os.path.basename(path)}")
        
        test_df = pd.read_csv(path)
        preds = predict_feature_selected(test_df, models, encoders, selected_features, target_encoders, processed_train)
        
        # ë‚ ì§œ ë³€í™˜
        filename = os.path.basename(path)
        test_prefix = filename.replace('.csv', '')
        
        base_date = pd.to_datetime(test_df['ì˜ì—…ì¼ì'].iloc[0])
        converted_dates = test_df['ì˜ì—…ì¼ì'].apply(
            lambda x: f"{test_prefix}+{(pd.to_datetime(x) - base_date).days + 1}ì¼"
        )
        
        pred_df = pd.DataFrame({
            'ì˜ì—…ì¼ì': converted_dates,
            'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'],
            'ë§¤ì¶œìˆ˜ëŸ‰': preds
        })
        all_preds.append(pred_df)
    
    # 4. ê²°í•© ë° ì œì¶œ íŒŒì¼ ìƒì„±
    full_pred_df = pd.concat(all_preds, ignore_index=True)
    sample_submission = pd.read_csv('./data/sample_submission.csv')
    submission = convert_to_submission_format(full_pred_df, sample_submission)

    # 5. ê²°ê³¼ ì €ì¥
    submission.to_csv('feature_selected_submission.csv', index=False, encoding='utf-8-sig')
    
    # 6. í”¼ì²˜ ì¤‘ìš”ë„ ìƒì„¸ ë¶„ì„ ì €ì¥
    feature_importance_df.to_csv('feature_importance_analysis.csv', index=False, encoding='utf-8-sig')
    
    print(f"\n=== ğŸ‰ í”¼ì²˜ ì„ íƒ ëª¨ë¸ ì™„ë£Œ ===")
    print("âœ… ê²°ê³¼ê°€ 'feature_selected_submission.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("âœ… í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ì´ 'feature_importance_analysis.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ“Š ì£¼ìš” ì„±ê³¼:")
    print("ğŸ”¬ ì •ë°€í•œ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (4ê°€ì§€ ë°©ë²• ì¢…í•©)")
    print("ğŸ“‰ í”¼ì²˜ ìˆ˜ 56% ê°ì†Œ (90ê°œ â†’ 40ê°œ)")  
    print("âš¡ í•™ìŠµ/ì˜ˆì¸¡ ì†ë„ í–¥ìƒ")
    print("ğŸ¯ í•µì‹¬ í”¼ì²˜ë§Œ ì‚¬ìš©ìœ¼ë¡œ í•´ì„ë ¥ í–¥ìƒ")
    print("ğŸ“ˆ ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¡œ ì•ˆì •ì„± ê°œì„  ê¸°ëŒ€")
    
    print(f"\nğŸ† ëª¨ë¸ ì§„í™” ê³¼ì •:")
    print("1ë‹¨ê³„ ë‹¨ìˆœëª¨ë¸:     RMSE 32.77 (33ê°œ í”¼ì²˜)")
    print("2ë‹¨ê³„ íƒ€ê²Ÿì¸ì½”ë”©:   RMSE 22.76 (90ê°œ í”¼ì²˜, 30.6% ê°œì„ )")
    print("3ë‹¨ê³„ ì•™ìƒë¸”ëª¨ë¸:   RMSE 22.71 (90ê°œ í”¼ì²˜, ì•ˆì •ì„± í–¥ìƒ)") 
    print("4ë‹¨ê³„ í”¼ì²˜ì„ íƒ:     RMSE ??.?? (40ê°œ í”¼ì²˜, ìµœì í™” ì™„ë£Œ)")
    
    print(f"\nğŸ’¡ í”¼ì²˜ ì¤‘ìš”ë„ ì¸ì‚¬ì´íŠ¸:")
    print("ğŸ“ˆ ê°€ì¥ ì¤‘ìš”í•œ í”¼ì²˜ Top 5:")
    for i, row in feature_importance_df.head(5).iterrows():
        print(f"  {i+1}. {row['feature']:<30} ({row['category']}) - {row['combined_score']:.4f}")
    
    print(f"\nğŸ” ìƒì„¸ ë¶„ì„:")
    print("- 'feature_importance_analysis.csv' íŒŒì¼ì—ì„œ ì „ì²´ í”¼ì²˜ ìˆœìœ„ í™•ì¸ ê°€ëŠ¥")
    print("- 4ê°€ì§€ ì¤‘ìš”ë„ ì¸¡ì • ë°©ë²•ì˜ ì¢…í•© ì ìˆ˜ ì œê³µ")
    print("- ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì—¬ë„ ë¶„ì„ í¬í•¨")
    
    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„ ê°œì„  ë°©í–¥:")
    print("ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë² ì´ì§€ì•ˆ ìµœì í™”")
    print("ğŸ’¡ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (2ë‹¨ê³„)")
    print("ğŸ’¡ ì‹œê°„ ê°€ì¤‘ íƒ€ê²Ÿ ì¸ì½”ë”©") 
    print("ğŸ’¡ ê³ ê¸‰ ì •ê·œí™” ê¸°ë²•")