import os
import glob
import random
import shutil # í´ë” ì‚­ì œë¥¼ ìœ„í•´ ì¶”ê°€
import numpy as np
import pandas as pd
from tqdm import tqdm
from autogluon.tabular import TabularPredictor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


# ì‹œë“œ ê³ ì •
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(42)

# --- ê¸°ì¡´ ëª¨ë¸ í´ë” ì‚­ì œ ---
# í”¼ì²˜ ë¡œì§ì´ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ì „ ë²„ì „ê³¼ ì¶©ëŒì„ ë§‰ê¸° ìœ„í•´ ê¸°ì¡´ ëª¨ë¸ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
# if os.path.exists('autogluon_models'):
#     print("í”¼ì²˜ ë¡œì§ ë³€ê²½ìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ëª¨ë¸ í´ë”('autogluon_models')ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
#     shutil.rmtree('autogluon_models')


# ê³µíœ´ì¼ ë¦¬ìŠ¤íŠ¸ ë° í•¨ìˆ˜
holiday_dates = pd.to_datetime([
    "2023-01-01","2023-01-21","2023-01-22","2023-01-23","2023-01-24",
    "2023-03-01","2023-05-05","2023-05-27","2023-05-29","2023-06-06",
    "2023-08-15","2023-09-28","2023-09-29","2023-09-30","2023-10-03",
    "2023-10-09","2023-12-25",
    "2024-01-01","2024-02-09","2024-02-10","2024-02-11","2024-02-12",
    "2024-03-01","2024-04-10","2024-05-05","2024-05-06","2024-05-15",
    "2024-06-06","2024-08-15","2024-09-16","2024-09-17","2024-09-18",
    "2024-10-03","2024-10-09","2024-12-25",
    "2025-01-01","2025-01-27","2025-01-28","2025-01-29","2025-01-30",
    "2025-03-01","2025-03-03","2025-05-05","2025-05-06","2025-06-03",
    "2025-06-06","2025-08-15","2025-10-03","2025-10-05","2025-10-06",
    "2025-10-07","2025-10-08","2025-10-09","2025-12-25"
])

def is_korean_holiday(date):
    return int(date in holiday_dates)


# --- ë©”ë‰´ ë©”íƒ€ë°ì´í„° (ì „ì—­ ë³€ìˆ˜) ---
menu_meta = {}
discontinued_menus = set()


# í”¼ì²˜ ìƒì„± í•¨ìˆ˜ (ê°œì„ )
def create_features(df):
    df = df.copy()
    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
    
    # 1. ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
    df['day_of_week'] = df['ì˜ì—…ì¼ì'].dt.weekday
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['month'] = df['ì˜ì—…ì¼ì'].dt.month
    df['day'] = df['ì˜ì—…ì¼ì'].dt.day
    df['week_of_year'] = df['ì˜ì—…ì¼ì'].dt.isocalendar().week.astype(int)
    df['year'] = df['ì˜ì—…ì¼ì'].dt.year
    df['is_holiday'] = df['ì˜ì—…ì¼ì'].apply(is_korean_holiday)

    # 2. ì˜ì—…ì¥/ë©”ë‰´ëª… ê´€ë ¨ í”¼ì²˜
    if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in df.columns:
        df['ì˜ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_').str[0]
        df['ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_').str[1:].apply(lambda x: '_'.join(x) if x else '')
        
        # ë©”ë‰´ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜)
        df['ë¶„ì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ê¼¬ì¹˜ì–´ë¬µ|ë–¡ë³¶ì´|í•«ë„ê·¸|íŠ€ê¹€|ìˆœëŒ€|ì–´ë¬µ|íŒŒì „', na=False).astype(int)
        df['ìŒë£Œë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ì•„ë©”ë¦¬ì¹´ë…¸|ë¼ë–¼|ì½œë¼|ìŠ¤í”„ë¼ì´íŠ¸|ìƒìˆ˜|ì—ì´ë“œ|í•˜ì´ë³¼|ìŒë£Œ|ì‹í˜œ', na=False, case=False).astype(int)
        df['ì£¼ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ë§‰ê±¸ë¦¬|ì†Œì£¼|ë§¥ì£¼|ì¹µí…Œì¼|ì™€ì¸|beer|ìƒë§¥ì£¼', na=False, case=False).astype(int)
        df['ì‹ì‚¬ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('êµ­ë°¥|í•´ì¥êµ­|ë¶ˆê³ ê¸°|ê¹€ì¹˜|ëœì¥|ë¹„ë¹”ë°¥|ê°ˆë¹„|ê³µê¹ƒë°¥|íŒŒìŠ¤íƒ€|í”¼ì|ìŠ¤í…Œì´í¬|ìƒëŸ¬ë“œ|ë¦¬ì¡°ë˜', na=False).astype(int)
        df['ë‹¨ì²´/ëŒ€ì—¬'] = df['ë©”ë‰´ëª…'].str.contains('ë‹¨ì²´|íŒ¨í‚¤ì§€|ì„¸íŠ¸|ëŒ€ì—¬ë£Œ|conference|convention', na=False, case=False).astype(int)
    
    # 3. ë©”ë‰´ë³„ íŠ¹ì„± í”¼ì²˜ (ë©”íƒ€ë°ì´í„° í™œìš©)
    if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in df.columns and menu_meta:
        # ì¶œì‹œ í›„ ê²½ê³¼ì¼ & í™œì„± ì‹œì¦Œ ì—¬ë¶€
        df['days_since_launch'] = -1
        df['is_active_month'] = 0
        
        for name, meta in menu_meta.items():
            menu_mask = (df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == name)
            if menu_mask.any():
                df.loc[menu_mask, 'days_since_launch'] = (df.loc[menu_mask, 'ì˜ì—…ì¼ì'] - meta['launch_date']).dt.days
                df.loc[menu_mask, 'is_active_month'] = df.loc[menu_mask, 'month'].isin(meta['active_months']).astype(int)

    # 4. ì‹œê³„ì—´ í”¼ì²˜ (Lag & Rolling)
    sort_keys = ['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ì˜ì—…ì¼ì'] if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in df.columns else ['ì˜ì—…ì¼ì']
    df = df.sort_values(sort_keys)
    
    if 'ë§¤ì¶œìˆ˜ëŸ‰' in df.columns:
        gb_key = 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in df.columns else 'ì˜ì—…ì¼ì'
        grouped = df.groupby(gb_key)['ë§¤ì¶œìˆ˜ëŸ‰']
        for lag in [1, 7, 14]:
            df[f'lag_{lag}'] = grouped.shift(lag)
        for window in [7, 14]:
            df[f'rolling_mean_{window}'] = grouped.shift(1).rolling(window).mean()
            df[f'rolling_std_{window}'] = grouped.shift(1).rolling(window).std()
    
    # 5. í›„ì²˜ë¦¬
    # AutoGluonì´ ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ì¸ì‹í•˜ë„ë¡ íƒ€ì… ë³€ê²½
    df['month'] = df['month'].astype(str)
    df['day'] = df['day'].astype(str)
    df['week_of_year'] = df['week_of_year'].astype(str)
    df['day_of_week'] = df['day_of_week'].astype(str)
    df['year'] = df['year'].astype(str)
    
    if 'ë©”ë‰´ëª…' in df.columns:
        df = df.drop(columns=['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…'])

    df = df.fillna(0)
    
    return df

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv('./data/train/train.csv')
train_df['ì˜ì—…ì¼ì'] = pd.to_datetime(train_df['ì˜ì—…ì¼ì'])
train_df['ë§¤ì¶œìˆ˜ëŸ‰'] = train_df['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)
submission_df = pd.read_csv('./data/sample_submission.csv')

# --- ë©”ë‰´ ë©”íƒ€ë°ì´í„° ìƒì„± ---
for name, group in train_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'):
    sales_data = group[group['ë§¤ì¶œìˆ˜ëŸ‰'] > 0]
    if not sales_data.empty:
        menu_meta[name] = {
            'launch_date': sales_data['ì˜ì—…ì¼ì'].min(),
            'last_sale_date': sales_data['ì˜ì—…ì¼ì'].max(),
            'active_months': list(sales_data['ì˜ì—…ì¼ì'].dt.month.unique())
        }

# ë‹¨ì¢… ë©”ë‰´ ì‹ë³„ (í•™ìŠµ ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œ ê¸°ì¤€ 60ì¼ ì´ìƒ íŒë§¤ ê¸°ë¡ ì—†ëŠ” ë©”ë‰´)
last_train_date = train_df['ì˜ì—…ì¼ì'].max()
discontinued_menus = {
    name for name, meta in menu_meta.items()
    if (last_train_date - meta['last_sale_date']).days > 60
}
print(f"ì´ {len(discontinued_menus)}ê°œì˜ ë‹¨ì¢… ì¶”ì • ë©”ë‰´ ì‹ë³„.")

# í”¼ì²˜ ìƒì„±
train_full_featured = create_features(train_df)

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
all_predictions = []

# test í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
test_paths = sorted(glob.glob('./data/test/*.csv'))
if not test_paths:
    print("test í´ë”ì— ì˜ˆì¸¡í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    # ê° ë©”ë‰´ë³„ë¡œ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
    unique_menus = train_full_featured['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].unique()
    training_logs = [] # í•™ìŠµ ë¡œê·¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for menu_name in tqdm(unique_menus, desc="ë©”ë‰´ë³„ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"):
        
        # ë‹¨ì¢… ë©”ë‰´ëŠ” ê±´ë„ˆë›°ê¸°
        if menu_name in discontinued_menus:
            print(f"ğŸ—‘ï¸ ë‹¨ì¢… ë©”ë‰´ë¡œ ì¶”ì •ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {menu_name}")
            continue

        # í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì‹ ê·œ ë©”ë‰´ ì²˜ë¦¬ (ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œ 0ìœ¼ë¡œ ì²˜ë¦¬)
        if menu_name not in menu_meta:
            print(f"ğŸ†• í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì‹ ê·œ ë©”ë‰´ì…ë‹ˆë‹¤: {menu_name}")
            continue
            
        # 1. ë©”ë‰´ë³„ ë°ì´í„° ì¤€ë¹„
        menu_train_data = train_full_featured[train_full_featured['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu_name].copy()
        
        if len(menu_train_data) < 30: # í•™ìŠµì— í•„ìš”í•œ ìµœì†Œ ë°ì´í„° ìˆ˜
            continue
            
        # AutoGluon í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        train_data_ag = menu_train_data.drop(columns=['ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])

        predictor_path = f'autogluon_models/{menu_name.replace("/", "_").replace(" ", "")}'
        # "ìŠ¤ë§ˆíŠ¸í•œ ì´ì–´í•˜ê¸°" ê¸°ëŠ¥: ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ í•™ìŠµ
        if os.path.exists(predictor_path):
            print(f"âœ… ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ë°œê²¬: {menu_name}. ë¶ˆëŸ¬ì˜¤ê¸°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            predictor = TabularPredictor.load(predictor_path)
        else:
            print(f"ğŸš€ ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ ì‹œì‘: {menu_name}")
            predictor = TabularPredictor(
                label='ë§¤ì¶œìˆ˜ëŸ‰',
                path=predictor_path,
                problem_type='regression',
                eval_metric='symmetric_mean_absolute_percentage_error'
            ).fit(
                train_data_ag,
                time_limit=60, # ë©”ë‰´ë³„ ìµœëŒ€ í•™ìŠµ ì‹œê°„(ì´ˆ)
                hyperparameters={
                    'CAT': {},  # CatBoost
                    'GBM': {},  # LightGBM (LGBê°€ ì•„ë‹Œ GBMì´ ì •í™•í•œ í‚¤ì…ë‹ˆë‹¤)
                    'XGB': {},  # XGBoost
                },
                ag_args_fit={'num_gpus': 0}
            )
        
        # ëª¨ë¸ ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ ì¶œë ¥ (SMAPE ê¸°ì¤€)
        print(f"--- {menu_name} ëª¨ë¸ ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ ---")
        leaderboard = predictor.leaderboard(silent=True)
        print(leaderboard[['model', 'score_val', 'fit_time']])
        print("-------------------------------------\n")
        
        # í•™ìŠµ ê²°ê³¼ ë¡œê¹…
        if not leaderboard.empty:
            best_model_info = leaderboard.iloc[0]
            fit_summary = predictor.fit_summary(verbosity=0)
            training_logs.append({
                'menu_name': menu_name,
                'training_samples': len(train_data_ag),
                'best_model': best_model_info['model'],
                'validation_smape': best_model_info['score_val'],
                'total_fit_time': fit_summary.get('total_time', 0) # .get()ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ê°€ ì—†ì„ ë•Œ ì˜¤ë¥˜ ë°©ì§€
            })

        # 3. ë©”ë‰´ë³„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ìˆœí™˜ ì˜ˆì¸¡)
        for path in test_paths:
            test_file_df = pd.read_csv(path)
            basename = os.path.basename(path).replace('.csv', '')

            # [ìˆ˜ì •] ìˆœí™˜ ì˜ˆì¸¡ ì‹œ ê³¼ê±° ë°ì´í„° êµ¬ì„± ë°©ì‹ì„ ìˆ˜ì •í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°”ë¡œì¡ìŠµë‹ˆë‹¤.
            # - ì›ì¸: í”¼ì²˜ê°€ ìƒì„±ëœ ë°ì´í„°(train_full_featured)ì™€ ì›ë³¸ ë°ì´í„°(test_file_df)ë¥¼ í•©ì³ ì»¬ëŸ¼ ë¶ˆì¼ì¹˜ ë°œìƒ
            # - í•´ê²°: í”¼ì²˜ ìƒì„± ì „ì˜ ì›ë³¸ train_dfë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
            # - ë˜í•œ, test íŒŒì¼ì— íŒë§¤ ê¸°ë¡ì´ ì—†ëŠ” ë©”ë‰´ë„ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ ê´€ë ¨ continue ë¡œì§ì„ ì œê±°í•©ë‹ˆë‹¤.
            historical_data = pd.concat([
                train_df[train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu_name], # ì›ë³¸ train_df ì‚¬ìš©
                test_file_df[test_file_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] == menu_name]
            ]).copy()
            
            historical_data['ì˜ì—…ì¼ì'] = pd.to_datetime(historical_data['ì˜ì—…ì¼ì'])
            historical_data = historical_data.sort_values(by='ì˜ì—…ì¼ì').tail(28) # ëŒ€íšŒ ê·œì¹™: ìµœê·¼ 28ì¼ ë°ì´í„° ì‚¬ìš©

            # 7ì¼ ì˜ˆì¸¡
            for i in range(7):
                last_date = historical_data['ì˜ì—…ì¼ì'].max()
                next_date = last_date + pd.Timedelta(days=1)
                
                # ì˜ˆì¸¡ì„ ìœ„í•œ ìƒˆë¡œìš´ í–‰ ìƒì„±
                new_row = pd.DataFrame([{'ì˜ì—…ì¼ì': next_date, 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': menu_name, 'ë§¤ì¶œìˆ˜ëŸ‰': np.nan}])
                
                # í”¼ì²˜ ìƒì„±ì„ ìœ„í•´ ê³¼ê±° ë°ì´í„°ì™€ í•©ì¹¨
                combined_for_feature = pd.concat([historical_data, new_row], ignore_index=True)
                featured_data = create_features(combined_for_feature)
                
                # ì˜ˆì¸¡í•  ë§ˆì§€ë§‰ í–‰ ì„ íƒ
                X_test = featured_data.tail(1).drop(columns=['ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ë§¤ì¶œìˆ˜ëŸ‰'])
                
                # ì˜ˆì¸¡
                pred = predictor.predict(X_test).iloc[0]
                pred = max(0, pred)
                
                # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
                all_predictions.append({
                    'ì˜ì—…ì¼ì': f"{basename}+{i+1}ì¼",
                    'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': menu_name,
                    'ë§¤ì¶œìˆ˜ëŸ‰': pred
                })

                # ë‹¤ìŒ ì˜ˆì¸¡ì„ ìœ„í•´ ì˜ˆì¸¡ê°’ì„ í¬í•¨í•˜ì—¬ historical_data ì—…ë°ì´íŠ¸
                # update_row = featured_data.tail(1).copy() -> ì´ ë°©ì‹ì€ í”¼ì²˜ê°€ í¬í•¨ëœ ë°ì´í„°ë¥¼ í•©ì¹˜ë¯€ë¡œ ì˜¤ë¥˜ ìœ ë°œ
                # update_row['ë§¤ì¶œìˆ˜ëŸ‰'] = pred
                # historical_data = pd.concat([historical_data, update_row], ignore_index=True)
                
                # [ìˆ˜ì •ëœ ë¡œì§] í•µì‹¬ ì •ë³´ë§Œ í¬í•¨ëœ í–‰ì„ ë§Œë“¤ì–´ì„œ historical_dataì— ì¶”ê°€
                new_prediction_row = new_row.copy()
                new_prediction_row['ë§¤ì¶œìˆ˜ëŸ‰'] = pred
                historical_data = pd.concat([historical_data, new_prediction_row], ignore_index=True)

# í•™ìŠµ ë¡œê·¸ ì €ì¥
if training_logs:
    log_df = pd.DataFrame(training_logs)
    log_df.to_csv('training_log.csv', index=False, encoding='utf-8-sig')
    print("âœ… í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ training_log.csv íŒŒì¼ ìƒì„± ì™„ë£Œ")

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
if submission_df is not None:
    # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¦
    if all_predictions:
        pred_df = pd.DataFrame(all_predictions)

        # 1. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜
        submission_pivot = pred_df.pivot(
            index='ì˜ì—…ì¼ì',
            columns='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…',
            values='ë§¤ì¶œìˆ˜ëŸ‰'
        )

        # 2. sample_submissionì„ ìµœì¢… ì œì¶œë³¸ìœ¼ë¡œ ë³µì‚¬í•˜ê³  ì¸ë±ìŠ¤ ì„¤ì •
        final_submission = submission_df.copy()
        final_submission = final_submission.set_index('ì˜ì—…ì¼ì')

        # 3. update ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ê°’ ì—…ë°ì´íŠ¸
        final_submission.update(submission_pivot)

        # 4. ì¸ë±ìŠ¤ ë¦¬ì…‹
        final_submission.reset_index(inplace=True)

    else:
        # ì˜ˆì¸¡ì´ ì—†ëŠ” ê²½ìš° sample_submissionì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        final_submission = submission_df.copy()

    # ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  NaN ê°’ì„ 0ìœ¼ë¡œ ì±„ì›€ (ì•ˆì „ì¥ì¹˜)
    final_submission = final_submission.fillna(0)
    
    final_submission.to_csv('submission_autogluon_per_item.csv', index=False, encoding='utf-8-sig')
    print("âœ… submission_autogluon_per_item.csv íŒŒì¼ ìƒì„± ì™„ë£Œ")
else:
    print("ìƒì„±ëœ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤.")

print("\n=== ğŸ† AutoGluon (ë©”ë‰´ë³„ ëª¨ë¸)ì„ ì´ìš©í•œ ìë™í™” ëª¨ë¸ ===")
print("âœ… ê° ë©”ë‰´ë³„ë¡œ ë…ë¦½ì ì¸ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ ì‹œë„")
print("âœ… AutoGluonì´ ìë™ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ íƒìƒ‰ ë° ì•™ìƒë¸” ìˆ˜í–‰")
print("âœ… ë°ì´í„° ê¸°ë°˜ì˜ ë©”ë‰´ íŠ¹ì„±(ì¶œì‹œì¼, íŒë§¤ì‹œì¦Œ, ë‹¨ì¢…)ì„ í”¼ì²˜ë¡œ í™œìš©")
print("\nğŸ¯ ìˆ˜ë£Œ ê¸°ì¤€ ëª©í‘œ:")
print("  â€¢ Public Score â‰¤ 0.711046")
print("  â€¢ Private Score â‰¤ 0.693935")
