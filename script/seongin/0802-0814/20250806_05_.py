import os
import random
import glob
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.feature_selection import SelectKBest, f_regression
from catboost import CatBoostRegressor
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# --- ê³ ì • ì‹œë“œ ---
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
set_seed(42)

# --- ê³µíœ´ì¼ 2023~2024 ---
holidays_2023_2024 = [
    "2023-01-01", "2023-01-21", "2023-01-22", "2023-01-23", "2023-01-24",
    "2023-03-01", "2023-05-05", "2023-05-27", "2023-05-29", "2023-06-06",
    "2023-08-15", "2023-09-28", "2023-09-29", "2023-09-30", "2023-10-03", 
    "2023-10-09", "2023-12-25",
    "2024-01-01", "2024-02-09", "2024-02-10", "2024-02-11", "2024-02-12", 
    "2024-03-01", "2024-04-10", "2024-05-05", "2024-05-06", "2024-05-15", 
    "2024-06-06", "2024-08-15", "2024-09-16", "2024-09-17", "2024-09-18", 
    "2024-10-03", "2024-10-09", "2024-12-25",
]

# --- ì•ˆì „í•œ ë¼ë²¨ ì¸ì½”ë”© í•¨ìˆ˜ ---
def safe_label_encode(le, values):
    """ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë¼ë²¨ ì¸ì½”ë”©"""
    result = []
    for val in values.astype(str):
        try:
            result.append(le.transform([val])[0])
        except ValueError:
            result.append(-1)  # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ëŠ” -1ë¡œ ì²˜ë¦¬
    return np.array(result)

# --- ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”© ---
def safe_target_encoding(train_df, test_df, cat_cols, target_col, alpha=10, cv_folds=5):
    """ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”© (êµì°¨ ê²€ì¦ ì‚¬ìš©)"""
    print(f"íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš©: {cat_cols}")
    
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    target_encoders = {}
    
    for col in cat_cols:
        if col not in train_df.columns:
            continue
            
        print(f"  - {col} ì¸ì½”ë”© ì¤‘...")
        train_encoded = np.zeros(len(train_df))
        
        # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì „í•œ íƒ€ê²Ÿ ì¸ì½”ë”©
        for train_idx, val_idx in kf.split(train_df):
            train_part = train_df.iloc[train_idx]
            val_part = train_df.iloc[val_idx]
            
            # ë² ì´ì§€ì•ˆ ìŠ¤ë¬´ë”©
            target_mean = train_part.groupby(col)[target_col].mean()
            global_mean = train_part[target_col].mean()
            count = train_part.groupby(col).size()
            
            smoothed = (target_mean * count + global_mean * alpha) / (count + alpha)
            train_encoded[val_idx] = val_part[col].map(smoothed).fillna(global_mean)
        
        train_df[f'{col}_target_enc'] = train_encoded
        
        # í…ŒìŠ¤íŠ¸ìš©: ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ê³„ì‚°
        target_mean = train_df.groupby(col)[target_col].mean()
        global_mean = train_df[target_col].mean()
        count = train_df.groupby(col).size()
        smoothed = (target_mean * count + global_mean * alpha) / (count + alpha)
        
        # ì¸ì½”ë” ì €ì¥
        target_encoders[col] = {
            'smoothed': smoothed.to_dict(),
            'global_mean': global_mean
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©
        if col in test_df.columns:
            test_df[f'{col}_target_enc'] = test_df[col].map(smoothed).fillna(global_mean)
        else:
            test_df[f'{col}_target_enc'] = global_mean
    
    return train_df, test_df, target_encoders

# --- ğŸŒ¡ï¸ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ ìƒì„± ---
def create_temperature_based_features(df):
    """ì˜¨ë„ íŠ¹ì„± ê¸°ë°˜ ê³„ì ˆë³„ ë©”ë‰´ í”¼ì²˜ ìƒì„±"""
    
    print("ğŸŒ¡ï¸ ì˜¨ë„ ê¸°ë°˜ ê³„ì ˆ ë©”ë‰´ í”¼ì²˜ ìƒì„± ì¤‘...")
    
    # ë©”ë‰´ë³„ ì˜¨ë„ íŠ¹ì„± ì •ì˜ (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜)
    menu_temperature_map = {
        # ëœ¨ê±°ìš´ ìŒì‹ (ê²¨ìš¸ ì„ í˜¸)
        'ëœ¨ê±°ìš´ìŒì‹': {
            'keywords': ['êµ­ë°¥', 'í•´ì¥êµ­', 'ì°Œê°œ', 'íƒ•', 'êµ­ìˆ˜', 'ë¼ë©´', 'ìš°ë™', 'ë”°ëœ»í•œ', 'ì˜¨', 'ëœ¨ê±°ìš´', 
                        'ê¹€ì¹˜ì°Œê°œ', 'ëœì¥ì°Œê°œ', 'ë¶€ëŒ€ì°Œê°œ', 'ìˆœë‘ë¶€', 'ì‚¼ê³„íƒ•', 'ê³°íƒ•', 'ì„¤ë íƒ•', 'ê°ˆë¹„íƒ•',
                        'ë¯¸ì†Œë¼ë©˜', 'ì§œì¥ë©´', 'ì§¬ë½•', 'ë¶ˆê³ ê¸°', 'ë–¡ë³¶ì´', 'ì–´ë¬µ', 'í˜¸ë–¡', 'ë¶•ì–´ë¹µ', 'êµ°ê³ êµ¬ë§ˆ'],
            'season_preference': {'ê²¨ìš¸': 1.5, 'ê°€ì„': 1.2, 'ë´„': 0.8, 'ì—¬ë¦„': 0.4}
        },
        
        # ì‹œì›í•œ ìŒì‹ (ì—¬ë¦„ ì„ í˜¸)  
        'ì‹œì›í•œìŒì‹': {
            'keywords': ['ëƒ‰ë©´', 'ë¬¼ëƒ‰ë©´', 'ë¹„ë¹”ëƒ‰ë©´', 'ëƒ‰êµ­ìˆ˜', 'ì½©êµ­ìˆ˜', 'ì•„ì´ìŠ¤í¬ë¦¼', 'ë¹™ìˆ˜', 'íŒ¥ë¹™ìˆ˜', 
                        'ëƒ‰ì»¤í”¼', 'ì•„ì´ìŠ¤', 'í”„ë¼í˜', 'ìŠ¤ë¬´ë””', 'ì–¼ìŒ', 'ì‹œì›í•œ', 'ì°¨ê°€ìš´', 'ëƒ‰', 
                        'ìƒëŸ¬ë“œ', 'ê³¼ì¼', 'ìš”ê±°íŠ¸', 'ì†Œë¥´ë² ', 'ì ¤ë¼í† '],
            'season_preference': {'ì—¬ë¦„': 1.5, 'ë´„': 1.2, 'ê°€ì„': 0.8, 'ê²¨ìš¸': 0.4}
        },
        
        # ë”°ëœ»í•œ ìŒë£Œ (ê²¨ìš¸ ì„ í˜¸)
        'ë”°ëœ»í•œìŒë£Œ': {
            'keywords': ['ì•„ë©”ë¦¬ì¹´ë…¸', 'ë¼ë–¼', 'ì¹´í‘¸ì¹˜ë…¸', 'ë§ˆí‚¤ì•„í† ', 'ëª¨ì¹´', 'í•«ì´ˆì½œë¦¿', 'ë°€í¬í‹°', 
                        'ì°¨', 'ë…¹ì°¨', 'í™ì°¨', 'í—ˆë¸Œì°¨', 'ìƒê°•ì°¨', 'ìœ ìì°¨', 'ê¿€ì°¨', 'ë”°ëœ»í•œ'],
            'season_preference': {'ê²¨ìš¸': 1.4, 'ê°€ì„': 1.3, 'ë´„': 0.9, 'ì—¬ë¦„': 0.5}
        },
        
        # ì‹œì›í•œ ìŒë£Œ (ì—¬ë¦„ ì„ í˜¸)
        'ì‹œì›í•œìŒë£Œ': {
            'keywords': ['ì½œë¼', 'ìŠ¤í”„ë¼ì´íŠ¸', 'ì‚¬ì´ë‹¤', 'ë§¥ì£¼', 'ì†Œì£¼', 'í•˜ì´ë³¼', 'ì¹µí…Œì¼', 
                        'ì—ì´ë“œ', 'ë ˆëª¨ë„¤ì´ë“œ', 'ìƒìˆ˜', 'íƒ„ì‚°ìˆ˜', 'ì£¼ìŠ¤', 'ì¥¬ìŠ¤'],
            'season_preference': {'ì—¬ë¦„': 1.4, 'ë´„': 1.1, 'ê°€ì„': 0.9, 'ê²¨ìš¸': 0.6}
        }
    }
    
    # ê° ë©”ë‰´ì˜ ì˜¨ë„ íŠ¹ì„± ë¶„ë¥˜
    for temp_type, info in menu_temperature_map.items():
        keywords = info['keywords']
        keyword_pattern = '|'.join(keywords)
        matches = df['ë©”ë‰´ëª…'].str.contains(keyword_pattern, case=False, na=False)
        df[f'ë©”ë‰´_{temp_type}'] = matches.astype(int)
    
    print(f"  ë©”ë‰´ ì˜¨ë„ ë¶„ë¥˜ ê²°ê³¼:")
    for temp_type in menu_temperature_map.keys():
        count = df[f'ë©”ë‰´_{temp_type}'].sum()
        print(f"    {temp_type}: {count:,}ê°œ ë©”ë‰´")
    
    # ì›”ë³„ ì˜¨ë„ ê°€ì¤‘ì¹˜ (ì‹¤ì œ í•œêµ­ ê¸°í›„ ë°˜ì˜)
    monthly_temp_weights = {
        1: -0.8, 2: -0.6, 3: 0.0, 4: 0.3, 5: 0.6, 6: 0.8,
        7: 1.0, 8: 1.0, 9: 0.5, 10: 0.2, 11: -0.2, 12: -0.6
    }
    
    df['ì›”ë³„_ì˜¨ë„ì ìˆ˜'] = df['ì›”'].map(monthly_temp_weights).fillna(0)
    
    # ì˜¨ë„ì™€ ë©”ë‰´ì˜ ì—°ì†ì  ë§¤ì¹­ ì ìˆ˜ (í•µì‹¬ í”¼ì²˜!)
    df['ì˜¨ë„_ë©”ë‰´_ë§¤ì¹­ì ìˆ˜'] = (
        df['ì›”ë³„_ì˜¨ë„ì ìˆ˜'] * df['ë©”ë‰´_ì‹œì›í•œìŒì‹'] * 1.0 +  # ë”ìš¸ìˆ˜ë¡ ì‹œì›í•œ ìŒì‹ ì„ í˜¸
        df['ì›”ë³„_ì˜¨ë„ì ìˆ˜'] * df['ë©”ë‰´_ì‹œì›í•œìŒë£Œ'] * 0.8 +
        (1 - df['ì›”ë³„_ì˜¨ë„ì ìˆ˜']) * df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * 1.0 +  # ì¶”ìš¸ìˆ˜ë¡ ëœ¨ê±°ìš´ ìŒì‹ ì„ í˜¸  
        (1 - df['ì›”ë³„_ì˜¨ë„ì ìˆ˜']) * df['ë©”ë‰´_ë”°ëœ»í•œìŒë£Œ'] * 0.8
    )
    
    # ê³„ì ˆë³„ ì˜¨ë„ íŠ¹ì„± ë§¤ì¹­ í”¼ì²˜ ìƒì„±
    seasons = ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']
    
    for temp_type, info in menu_temperature_map.items():
        season_pref = info['season_preference']
        
        for season in seasons:
            if season in df.columns:
                preference_score = season_pref.get(season, 1.0)
                df[f'{season}_{temp_type}_ë§¤ì¹­'] = (
                    df[season] * df[f'ë©”ë‰´_{temp_type}'] * preference_score
                )
    
    # ì˜¨ë„ ëŒ€ë¹„ í”¼ì²˜ (ë°˜ëŒ€ ê³„ì ˆ íš¨ê³¼)
    df['ê²¨ìš¸_ì‹œì›í•œìŒì‹_ëŒ€ë¹„'] = df['ê²¨ìš¸'] * df['ë©”ë‰´_ì‹œì›í•œìŒì‹'] * 0.3
    df['ì—¬ë¦„_ëœ¨ê±°ìš´ìŒì‹_ëŒ€ë¹„'] = df['ì—¬ë¦„'] * df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * 0.4
    df['ì—¬ë¦„_ë”°ëœ»í•œìŒë£Œ_ëŒ€ë¹„'] = df['ì—¬ë¦„'] * df['ë©”ë‰´_ë”°ëœ»í•œìŒë£Œ'] * 0.5
    df['ê²¨ìš¸_ì‹œì›í•œìŒë£Œ_ëŒ€ë¹„'] = df['ê²¨ìš¸'] * df['ë©”ë‰´_ì‹œì›í•œìŒë£Œ'] * 0.6
    
    # ì˜ì—…ì¥ë³„ ì˜¨ë„ íŠ¹ì„± (ê° ì˜ì—…ì¥ì˜ íŠ¹ìƒ‰ ë°˜ì˜)
    stores = ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']
    
    for store in stores:
        if f'ì˜ì—…ì¥_{store}' in df.columns:
            if store == 'í™”ë‹´ìˆ²ì£¼ë§‰':  # ì „í†µ ì£¼ë§‰ â†’ ë”°ëœ»í•œ ìŒì‹ íŠ¹í™”
                df[f'{store}_ëœ¨ê±°ìš´ìŒì‹_íŠ¹í™”'] = df[f'ì˜ì—…ì¥_{store}'] * df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * 1.3
                df[f'{store}_ê²¨ìš¸_ì‹œë„ˆì§€'] = df[f'ì˜ì—…ì¥_{store}'] * df['ê²¨ìš¸'] * 1.2
                
            elif store == 'í¬ë ˆìŠ¤íŠ¸ë¦¿':  # ê´€ê´‘ì§€ â†’ ê³„ì ˆì„± ê°•í•¨
                df[f'{store}_ê³„ì ˆë§¤ì¹­_ê°•í™”'] = (
                    df[f'ì˜ì—…ì¥_{store}'] * (
                        df['ì—¬ë¦„_ì‹œì›í•œìŒì‹_ë§¤ì¹­'] + df['ê²¨ìš¸_ëœ¨ê±°ìš´ìŒì‹_ë§¤ì¹­']
                    )
                )
                
            elif store == 'ì¹´í˜í…Œë¦¬ì•„':  # ì¹´í˜ â†’ ìŒë£Œ íŠ¹í™”
                df[f'{store}_ìŒë£Œ_ê³„ì ˆë§¤ì¹­'] = (
                    df[f'ì˜ì—…ì¥_{store}'] * (
                        df['ì—¬ë¦„_ì‹œì›í•œìŒë£Œ_ë§¤ì¹­'] + df['ê²¨ìš¸_ë”°ëœ»í•œìŒë£Œ_ë§¤ì¹­']
                    )
                )
    
    # ê·¹ë‹¨ì  ê³„ì ˆ ìƒí™© í”¼ì²˜
    df['í˜¹í•œê¸°_ëœ¨ê±°ìš´ìŒì‹'] = df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * (df['ì›”'].isin([12, 1, 2])).astype(int) * 1.5
    df['í˜¹ì„œê¸°_ì‹œì›í•œìŒì‹'] = df['ë©”ë‰´_ì‹œì›í•œìŒì‹'] * (df['ì›”'].isin([7, 8])).astype(int) * 1.5
    
    # íŠ¹ë³„í•œ ë‚ ì”¨ ì´ë²¤íŠ¸ (í•œêµ­ ê¸°í›„ íŠ¹ì„±)
    df['ì¥ë§ˆì² _ì‹¤ë‚´ìŒì‹'] = (df['ì›”'].isin([6, 7])).astype(int) * df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * 1.2
    df['í­ì—¼_ì‹œì›ìŒì‹'] = (df['ì›”'].isin([7, 8])).astype(int) * df['ë©”ë‰´_ì‹œì›í•œìŒì‹'] * 1.5
    df['í•œíŒŒ_ë”°ëœ»ìŒì‹'] = (df['ì›”'].isin([12, 1, 2])).astype(int) * df['ë©”ë‰´_ëœ¨ê±°ìš´ìŒì‹'] * 1.3
    
    print(f"  ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ ìƒì„± ì™„ë£Œ!")
    
    return df

# --- ê³ ê¸‰ ì‹œê°„ í”¼ì²˜ ìƒì„± ---
def create_advanced_time_features(df):
    """ê³ ê¸‰ ì‹œê°„ í”¼ì²˜ ìƒì„±"""
    df['ì˜ì—…ì¼ì_dt'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
    
    # ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
    df['ë…„'] = df['ì˜ì—…ì¼ì_dt'].dt.year
    df['ì›”'] = df['ì˜ì—…ì¼ì_dt'].dt.month
    df['ì¼'] = df['ì˜ì—…ì¼ì_dt'].dt.day
    df['ìš”ì¼'] = df['ì˜ì—…ì¼ì_dt'].dt.dayofweek
    df['ì£¼ì°¨'] = df['ì˜ì—…ì¼ì_dt'].dt.isocalendar().week
    df['ë¶„ê¸°'] = df['ì˜ì—…ì¼ì_dt'].dt.quarter
    df['ë…„ì¤‘_ëª‡ì§¸ë‚ '] = df['ì˜ì—…ì¼ì_dt'].dt.dayofyear
    
    # ì‚¼ê°í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ì£¼ê¸°ì„± í‘œí˜„
    df['ì›”_sin'] = np.sin(2 * np.pi * df['ì›”'] / 12)
    df['ì›”_cos'] = np.cos(2 * np.pi * df['ì›”'] / 12)
    df['ìš”ì¼_sin'] = np.sin(2 * np.pi * df['ìš”ì¼'] / 7)
    df['ìš”ì¼_cos'] = np.cos(2 * np.pi * df['ìš”ì¼'] / 7)
    df['ì¼_sin'] = np.sin(2 * np.pi * df['ì¼'] / 31)
    df['ì¼_cos'] = np.cos(2 * np.pi * df['ì¼'] / 31)
    
    # ê³„ì ˆì„±
    df['ë´„'] = df['ì›”'].isin([3, 4, 5]).astype(int)
    df['ì—¬ë¦„'] = df['ì›”'].isin([6, 7, 8]).astype(int)
    df['ê°€ì„'] = df['ì›”'].isin([9, 10, 11]).astype(int)
    df['ê²¨ìš¸'] = df['ì›”'].isin([12, 1, 2]).astype(int)
    
    # íŠ¹ë³„í•œ ì›”
    df['1ì›”'] = (df['ì›”'] == 1).astype(int)
    df['3ì›”'] = (df['ì›”'] == 3).astype(int)
    df['5ì›”'] = (df['ì›”'] == 5).astype(int)
    df['8ì›”'] = (df['ì›”'] == 8).astype(int)
    df['12ì›”'] = (df['ì›”'] == 12).astype(int)
    
    # ì›”ì˜ íŠ¹ì„±
    df['ì›”ì´ˆ'] = (df['ì¼'] <= 5).astype(int)
    df['ì›”ì¤‘'] = ((df['ì¼'] > 5) & (df['ì¼'] <= 25)).astype(int)
    df['ì›”ë§'] = (df['ì¼'] > 25).astype(int)
    df['ê¸‰ì—¬ì¼ê·¼ì²˜'] = ((df['ì¼'] >= 23) & (df['ì¼'] <= 28)).astype(int)
    
    # ì£¼ë§/ê³µíœ´ì¼
    df['ì£¼ë§ì—¬ë¶€'] = (df['ìš”ì¼'] >= 5).astype(int)
    df['ê³µíœ´ì¼ì—¬ë¶€'] = df['ì˜ì—…ì¼ì_dt'].dt.strftime('%Y-%m-%d').isin(holidays_2023_2024).astype(int)
    df['ê¸ˆìš”ì¼'] = (df['ìš”ì¼'] == 4).astype(int)
    df['ì¼ìš”ì¼'] = (df['ìš”ì¼'] == 6).astype(int)
    
    return df

# --- ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ---
def create_interaction_features(df):
    """ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± (ì˜¨ë„ ê¸°ë°˜ í¬í•¨)"""
    # ê¸°ì¡´ ìƒí˜¸ì‘ìš© í”¼ì²˜ë“¤
    seasons = ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']
    stores = ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']
    
    for season in seasons:
        for store in stores:
            if f'{season}' in df.columns and f'ì˜ì—…ì¥_{store}' in df.columns:
                df[f'{store}_{season}'] = df[f'{season}'] * df[f'ì˜ì—…ì¥_{store}']
    
    # ë©”ë‰´ ì¹´í…Œê³ ë¦¬ Ã— ìš”ì¼
    menu_categories = ['ìŒë£Œë¥˜', 'ì£¼ë¥˜ë¥˜', 'ë¶„ì‹ë¥˜', 'í•œì‹ë¥˜', 'ì–‘ì‹ë¥˜', 'ë””ì €íŠ¸ë¥˜']
    for category in menu_categories:
        if category in df.columns:
            df[f'{category}_ì£¼ë§'] = df[category] * df['ì£¼ë§ì—¬ë¶€']
            df[f'{category}_í‰ì¼'] = df[category] * (1 - df['ì£¼ë§ì—¬ë¶€'])
    
    # ì˜ì—…ì¥ Ã— ì£¼ë§
    for store in stores:
        if f'ì˜ì—…ì¥_{store}' in df.columns:
            df[f'{store}_ì£¼ë§'] = df[f'ì˜ì—…ì¥_{store}'] * df['ì£¼ë§ì—¬ë¶€']
    
    # ì›” Ã— ì£¼ë§
    special_months = [1, 3, 5, 8, 12]
    for month in special_months:
        if f'{month}ì›”' in df.columns:
            df[f'{month}ì›”_ì£¼ë§'] = df[f'{month}ì›”'] * df['ì£¼ë§ì—¬ë¶€']
    
    return df

# --- ì´ìƒì¹˜ ì²˜ë¦¬ ---
def handle_outliers_by_group(df, target_col='ë§¤ì¶œìˆ˜ëŸ‰'):
    """ê·¸ë£¹ë³„ ì´ìƒì¹˜ ì²˜ë¦¬"""
    if target_col not in df.columns:
        return df
    
    print("ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
    original_mean = df[target_col].mean()
    
    # ì˜ì—…ì¥ë³„ ì´ìƒì¹˜ ì²˜ë¦¬
    df[f'{target_col}_clipped'] = df.groupby('ì˜ì—…ì¥ëª…')[target_col].transform(
        lambda x: x.clip(lower=x.quantile(0.02), upper=x.quantile(0.98))
    )
    
    # ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
    processed_mean = df[f'{target_col}_clipped'].mean()
    print(f"  ì´ìƒì¹˜ ì²˜ë¦¬ ì „ í‰ê· : {original_mean:.2f}")
    print(f"  ì´ìƒì¹˜ ì²˜ë¦¬ í›„ í‰ê· : {processed_mean:.2f}")
    
    # ì›ë³¸ ëŒ€ì‹  ì²˜ë¦¬ëœ ê°’ ì‚¬ìš©
    df[target_col] = df[f'{target_col}_clipped']
    df = df.drop(f'{target_col}_clipped', axis=1)
    
    return df

# --- í†µí•© í”¼ì²˜ ìƒì„± ---
def create_ultimate_features(df, is_train=True, encoders=None, target_encoders=None, train_df_for_target=None):
    """ëª¨ë“  ê³ ê¸‰ í”¼ì²˜ë¥¼ í†µí•©í•œ ìµœì¢… í”¼ì²˜ ìƒì„±"""
    df = df.copy()
    
    # 1. ê³ ê¸‰ ì‹œê°„ í”¼ì²˜
    df = create_advanced_time_features(df)
    
    # 2. ì˜ì—…ì¥/ë©”ë‰´ ë¶„ë¦¬
    df['ì˜ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_').str[0]
    df['ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_', n=1).str[1].fillna('')
    
    # 3. ì˜ì—…ì¥ë³„ ì›í•« ì¸ì½”ë”©
    unique_stores = ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']
    for store in unique_stores:
        df[f'ì˜ì—…ì¥_{store}'] = (df['ì˜ì—…ì¥ëª…'] == store).astype(int)
    
    # 4. ë©”ë‰´ ì¹´í…Œê³ ë¦¬
    df['ìŒë£Œë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ì•„ë©”ë¦¬ì¹´ë…¸|ë¼ë–¼|ì½œë¼|ìŠ¤í”„ë¼ì´íŠ¸|ìƒìˆ˜|ì—ì´ë“œ|ìŒë£Œ', na=False).astype(int)
    df['ì£¼ë¥˜ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ë§‰ê±¸ë¦¬|ì†Œì£¼|ë§¥ì£¼|ì™€ì¸|ì¹µí…Œì¼|í•˜ì´ë³¼', na=False).astype(int)
    df['ë¶„ì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ë–¡ë³¶ì´|íŠ€ê¹€|í•«ë„ê·¸|ì–´ë¬µ|ê¼¬ì¹˜', na=False).astype(int)
    df['í•œì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('êµ­ë°¥|ë¶ˆê³ ê¸°|ê¹€ì¹˜|ëœì¥|ë¹„ë¹”ë°¥|í•œì‹', na=False).astype(int)
    df['ì–‘ì‹ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('íŒŒìŠ¤íƒ€|í”¼ì|ìŠ¤í…Œì´í¬|ìƒëŸ¬ë“œ|ì–‘ì‹', na=False).astype(int)
    df['ë””ì €íŠ¸ë¥˜'] = df['ë©”ë‰´ëª…'].str.contains('ì¼€ì´í¬|ë¹µ|ê³¼ì|ë””ì €íŠ¸|ì•„ì´ìŠ¤í¬ë¦¼', na=False).astype(int)
    df['ì„¸íŠ¸ë©”ë‰´'] = df['ë©”ë‰´ëª…'].str.contains('ì„¸íŠ¸|íŒ¨í‚¤ì§€|ì½¤ë³´', na=False).astype(int)
    
    # 5. ğŸŒ¡ï¸ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ ìƒì„± (í•µì‹¬!)
    df = create_temperature_based_features(df)
    
    # 6. ìƒí˜¸ì‘ìš© í”¼ì²˜
    df = create_interaction_features(df)
    
    # 7. ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    categorical_features = ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
    
    if is_train:
        # í›ˆë ¨ ì‹œ: ìƒˆë¡œìš´ ì¸ì½”ë” ìƒì„±
        if encoders is None:
            encoders = {}
        for col in categorical_features:
            if col in df.columns:
                le = LabelEncoder()
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
                encoders[col] = le
        
        # ì´ìƒì¹˜ ì²˜ë¦¬
        df = handle_outliers_by_group(df, 'ë§¤ì¶œìˆ˜ëŸ‰')
        
        return df, encoders
    else:
        # í…ŒìŠ¤íŠ¸ ì‹œ: ê¸°ì¡´ ì¸ì½”ë” ì‚¬ìš©
        for col in categorical_features:
            if col in df.columns and encoders and col in encoders:
                df[f'{col}_encoded'] = safe_label_encode(encoders[col], df[col])
            else:
                df[f'{col}_encoded'] = -1
        
        # íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš© (í…ŒìŠ¤íŠ¸)
        if target_encoders and train_df_for_target is not None:
            target_cols = ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
            _, df, _ = safe_target_encoding(train_df_for_target, df, target_cols, 'ë§¤ì¶œìˆ˜ëŸ‰')
        
        return df

# --- ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ---
def train_ensemble_model_ultimate(train_df):
    """ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "="*80)
    print("ğŸš€ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ í¬í•¨ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
    print("="*80)
    
    # 1. í”¼ì²˜ ìƒì„±
    print("1ï¸âƒ£ ê³ ê¸‰ í”¼ì²˜ ìƒì„± ì¤‘...")
    train_df_processed, encoders = create_ultimate_features(train_df, is_train=True)
    
    # 2. í”¼ì²˜ ì„ íƒ
    print("2ï¸âƒ£ í”¼ì²˜ ì„ íƒ ì¤‘...")
    feature_columns = [col for col in train_df_processed.columns 
                      if col not in ['ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ë§¤ì¶œìˆ˜ëŸ‰', 'ì˜ì—…ì¼ì_dt']]
    
    # ìˆ˜ì¹˜í˜• í”¼ì²˜ì™€ ì¹´í…Œê³ ë¦¬í˜• í”¼ì²˜ ë¶„ë¦¬
    numeric_features = [col for col in feature_columns if col not in ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…']]
    categorical_features = [col for col in feature_columns if col in ['ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…']]
    
    # 3. ëª¨ë¸ í•™ìŠµ
    print("3ï¸âƒ£ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    models = {}
    
    # CatBoost
    print("  - CatBoost í•™ìŠµ ì¤‘...")
    catboost_model = CatBoostRegressor(
        iterations=1000,
        learning_rate=0.1,
        depth=8,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=100
    )
    catboost_model.fit(
        train_df_processed[numeric_features],
        train_df_processed['ë§¤ì¶œìˆ˜ëŸ‰'],
        cat_features=[i for i, col in enumerate(numeric_features) if col in categorical_features]
    )
    models['catboost'] = catboost_model
    
    # LightGBM
    print("  - LightGBM í•™ìŠµ ì¤‘...")
    lgb_model = lgb.LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.1,
        max_depth=8,
        num_leaves=31,
        random_state=42,
        verbose=-1
    )
    lgb_model.fit(train_df_processed[numeric_features], train_df_processed['ë§¤ì¶œìˆ˜ëŸ‰'])
    models['lightgbm'] = lgb_model
    
    # XGBoost
    print("  - XGBoost í•™ìŠµ ì¤‘...")
    xgb_model = xgb.XGBRegressor(
        n_estimators=1000,
        learning_rate=0.1,
        max_depth=8,
        random_state=42,
        verbosity=0
    )
    xgb_model.fit(train_df_processed[numeric_features], train_df_processed['ë§¤ì¶œìˆ˜ëŸ‰'])
    models['xgboost'] = xgb_model
    
    print("âœ… ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    return models, encoders, numeric_features, train_df_processed

# --- ê³ ê¸‰ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ---
def analyze_feature_importance_ultimate(models, feature_names, train_df_processed, target_col):
    """ì˜¨ë„ í”¼ì²˜ í¬í•¨ í¬ê´„ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„"""
    print("\n" + "="*80)
    print("ğŸ” ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ í¬í•¨ í¬ê´„ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„")
    print("="*80)
    
    importance_results = {}
    
    # 1. ê° ëª¨ë¸ë³„ í”¼ì²˜ ì¤‘ìš”ë„
    for model_name, model in models.items():
        if hasattr(model, 'feature_importances_'):
            importance_results[model_name] = model.feature_importances_
    
    # 2. í†µê³„ì  ì¤‘ìš”ë„ (F-score)
    X = train_df_processed[feature_names]
    y = train_df_processed[target_col]
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in feature_names:
        if col.endswith('_encoded'):
            X[col] = X[col].fillna(-1)
        else:
            X[col] = X[col].fillna(0)
    
    f_scores, _ = f_regression(X, y)
    importance_results['f_score'] = f_scores
    
    # 3. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„
    correlations = []
    for col in feature_names:
        if col in X.columns:
            corr = abs(X[col].corr(y))
            correlations.append(corr)
        else:
            correlations.append(0)
    importance_results['correlation'] = np.array(correlations)
    
    # 4. ì¢…í•© ì¤‘ìš”ë„ ê³„ì‚° (ê°€ì¤‘í‰ê· )
    weights = {
        'catboost': 0.35,
        'lightgbm': 0.3,
        'xgboost': 0.25,
        'f_score': 0.1
    }
    
    # ì •ê·œí™”
    normalized_importance = {}
    for method, scores in importance_results.items():
        if method != 'correlation':
            scores_norm = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
            normalized_importance[method] = scores_norm
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    combined_score = np.zeros(len(feature_names))
    for method, weight in weights.items():
        if method in normalized_importance:
            combined_score += weight * normalized_importance[method]
    
    # í”¼ì²˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'combined_score': combined_score,
        'catboost_importance': importance_results.get('catboost', np.zeros(len(feature_names))),
        'lightgbm_importance': importance_results.get('lightgbm', np.zeros(len(feature_names))),
        'xgboost_importance': importance_results.get('xgboost', np.zeros(len(feature_names))),
        'f_score': importance_results.get('f_score', np.zeros(len(feature_names))),
        'correlation': importance_results.get('correlation', np.zeros(len(feature_names)))
    }).sort_values('combined_score', ascending=False).reset_index(drop=True)
    
    # í”¼ì²˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì˜¨ë„ ê¸°ë°˜ ì¶”ê°€)
    def categorize_feature_ultimate(feature_name):
        if 'target_enc' in feature_name:
            return 'ğŸ¯ íƒ€ê²Ÿ ì¸ì½”ë”©'
        elif 'encoded' in feature_name:
            return 'ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©'
        elif any(temp in feature_name for temp in ['ëœ¨ê±°ìš´', 'ì‹œì›í•œ', 'ë”°ëœ»í•œ', 'ì˜¨ë„', 'ë§¤ì¹­', 'ëŒ€ë¹„', 'íŠ¹í™”', 'ì‹œë„ˆì§€', 'í˜¹í•œ', 'í˜¹ì„œ', 'ì¥ë§ˆ', 'í­ì—¼', 'í•œíŒŒ']):
            return 'ğŸŒ¡ï¸ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜'
        elif any(time in feature_name for time in ['ë…„', 'ì›”', 'ì¼', 'ìš”ì¼', 'ì£¼ì°¨', 'ë¶„ê¸°', 'ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']):
            return 'â° ì‹œê°„ í”¼ì²˜'
        elif any(store in feature_name for store in ['í¬ë ˆìŠ¤íŠ¸ë¦¿', 'ì¹´í˜í…Œë¦¬ì•„', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']):
            return 'ğŸª ì˜ì—…ì¥ í”¼ì²˜'
        elif any(cat in feature_name for cat in ['ìŒë£Œë¥˜', 'ì£¼ë¥˜ë¥˜', 'ë¶„ì‹ë¥˜', 'í•œì‹ë¥˜', 'ì–‘ì‹ë¥˜', 'ë””ì €íŠ¸ë¥˜', 'ì„¸íŠ¸ë©”ë‰´']):
            return 'ğŸ½ï¸ ë©”ë‰´ ì¹´í…Œê³ ë¦¬'
        elif 'sin' in feature_name or 'cos' in feature_name:
            return 'ğŸ“Š ì£¼ê¸°ì„± í”¼ì²˜'
        else:
            return 'ğŸ“ˆ ê¸°íƒ€ í”¼ì²˜'
    
    feature_importance_df['category'] = feature_importance_df['feature'].apply(categorize_feature_ultimate)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ë¶„ì„
    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í”¼ì²˜ ì¤‘ìš”ë„:")
    category_importance = feature_importance_df.groupby('category')['combined_score'].mean().sort_values(ascending=False)
    for category, score in category_importance.items():
        print(f"  {category}: {score:.4f}")
    
    # ìƒìœ„ í”¼ì²˜ ì¶œë ¥
    print(f"\nğŸ† ìƒìœ„ 20ê°œ í”¼ì²˜:")
    top_features = feature_importance_df.head(20)
    for idx, row in top_features.iterrows():
        print(f"  {idx+1:2d}. {row['feature']:<30} (ì ìˆ˜: {row['combined_score']:.4f}, ì¹´í…Œê³ ë¦¬: {row['category']})")
    
    return feature_importance_df

# --- ì•™ìƒë¸” ì˜ˆì¸¡ ---
def predict_ensemble_ultimate(test_df, models, encoders, feature_columns):
    """ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ë¥¼ í¬í•¨í•œ ì•™ìƒë¸” ì˜ˆì¸¡"""
    # í”¼ì²˜ ìƒì„±
    test_df_processed = create_ultimate_features(test_df, is_train=False, encoders=encoders)
    
    # ì˜ˆì¸¡
    predictions = {}
    for name, model in models.items():
        pred = model.predict(test_df_processed[feature_columns])
        predictions[name] = pred
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )
    weights = {'catboost': 0.4, 'lightgbm': 0.35, 'xgboost': 0.25}
    ensemble_pred = np.zeros(len(test_df))
    
    for name, pred in predictions.items():
        ensemble_pred += weights[name] * pred
    
    return ensemble_pred

# --- ì œì¶œ íŒŒì¼ ë³€í™˜ ---
def convert_to_submission_format_ultimate(pred_df, sample_submission):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì •ìˆ˜ ë³€í™˜ í¬í•¨)"""
    def convert_to_integer(value):
        if value < 0:
            return 0
        return max(0, round(value))
    
    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    pred_dict = {}
    for _, row in pred_df.iterrows():
        date = row['ì˜ì—…ì¼ì']
        menu = row['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']
        value = convert_to_integer(row['ë§¤ì¶œìˆ˜ëŸ‰'])
        pred_dict[(date, menu)] = value
    
    # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ í˜•ì‹ì— ë§ì¶° ê²°ê³¼ ìƒì„±
    final_df = sample_submission.copy()
    
    for idx, row in final_df.iterrows():
        date = row['ì˜ì—…ì¼ì']
        for col in final_df.columns[1:]:  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(ì˜ì—…ì¼ì) ì œì™¸
            value = pred_dict.get((date, col), 0)
            final_df.at[idx, col] = value
    
    return final_df

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸŒ¡ï¸ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
    train = pd.read_csv('./data/train/train.csv')
    print(f"   í›ˆë ¨ ë°ì´í„°: {train.shape}")
    
    # 2. ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
    models, encoders, feature_columns, train_df_processed = train_ensemble_model_ultimate(train)
    
    # 3. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    print("\n4ï¸âƒ£ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘...")
    feature_importance_df = analyze_feature_importance_ultimate(
        models, feature_columns, train_df_processed, 'ë§¤ì¶œìˆ˜ëŸ‰'
    )
    
    # 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    print("\n5ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    sample_submission = pd.read_csv('./data/sample_submission.csv')
    all_preds = []
    
    for test_file in sorted(glob.glob('./data/test/TEST_*.csv')):
        print(f"   ì²˜ë¦¬ ì¤‘: {test_file}")
        test_df = pd.read_csv(test_file)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = predict_ensemble_ultimate(test_df, models, encoders, feature_columns)
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (TEST_XX+Nì¼ í˜•ì‹ìœ¼ë¡œ)
        filename = os.path.basename(test_file)
        test_prefix = filename.replace('.csv', '')
        base_date = pd.to_datetime(test_df['ì˜ì—…ì¼ì'].iloc[0])
        
        converted_dates = test_df['ì˜ì—…ì¼ì'].apply(
            lambda x: f"{test_prefix}+{(pd.to_datetime(x) - base_date).days + 1}ì¼"
        )
        
        # ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±
        pred_df = pd.DataFrame({
            'ì˜ì—…ì¼ì': converted_dates,
            'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'],
            'ë§¤ì¶œìˆ˜ëŸ‰': predictions
        })
        all_preds.append(pred_df)
    
    # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ í•©ì¹˜ê¸°
    full_pred_df = pd.concat(all_preds, ignore_index=True)
    
    # 5. ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n6ï¸âƒ£ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    submission = convert_to_submission_format_ultimate(full_pred_df, sample_submission)
    
    # 6. ê²°ê³¼ ì €ì¥
    output_file = 'temperature_ensemble_submission.csv'
    submission.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 7. ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"   - ì´ ì˜ˆì¸¡ í–‰ ìˆ˜: {len(submission)}")
    print(f"   - ì´ ë©”ë‰´ ìˆ˜: {len(submission.columns) - 1}")
    print(f"   - í‰ê·  ì˜ˆì¸¡ê°’: {submission.iloc[:, 1:].values.mean():.2f}")
    print(f"   - ìµœëŒ€ ì˜ˆì¸¡ê°’: {submission.iloc[:, 1:].values.max():.2f}")
    print(f"   - ìµœì†Œ ì˜ˆì¸¡ê°’: {submission.iloc[:, 1:].values.min():.2f}")
    
    print("\nğŸ‰ ì˜¨ë„ ê¸°ë°˜ í”¼ì²˜ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ!")